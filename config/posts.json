{
  "posts": [
    {
      "content": "# **个人信息**\n\n- 周三甫/男/1998                                  Email：215817301@qq.com\n- 大学专业：软件开发Java方向（2017～2019）\n- 工作年限：2年（2019～2021）\n- 技术博客：https://zhousanfu.github.io\n- Github：https://github.com/zhousanfu (有原创repo)\n\n---\n\n# **工作经历**\n\n## **佛山哈啰信息技术服务有限公司（ 2019年7月～2021年5月 ）**\n\n- 负责各类数据平台的搭建及系统开发实现和系统间接口数据标准规范制定;\n- 大数据分析、处理、挖掘等系统的设计和开发和构建基于用户行为特征的平台化画像服务能力，并建立用户画像产品的评估机制和监控体系;\n- 参与项目的技术攻坚和优化，相关系统的架构设计和评审，以及对问题的跟踪和解决;\n\n---\n\n# **项目**\n\n### 网络舆情监控\n\n根据信息的语料库与报警监控信息库进行分析及告警，以确保信息的舆论健康发展；自动抽取的能准确代表文章主题思想的智能摘要，以快速了解文章大意与核心内容，提高用户信息利用效率。\n\n- Django、爬虫、情感分析、分词\n\n### 文本语义分类**模型**\n\n- 基于BERT与Tensorflow RNN的文本二分类与多分类模型，独立负责训练数据提取预处理、建模、训练、预测、调参优化、API服务部署；准确率在一定数据范围内达95%以上。使用方向：新闻分类，数据分析预处理、内容审核；\n- Tensorflow、NLP\n\n### 色情链接检测\n\n- 自动识别社交产品内生产的违规链接内容，减少产品风险；\n- 图像分割cv模型\n\n---\n\n# 开源项目和作品\n\n## 开源项目\n\n- [NLP文本分类模型](https://github.com/zhousanfu/bert_classifier)：项目的简要说明，Star和Fork数多的可以注明\n- [网络舆情监测](https://github.com/zhousanfu/digital_monitoring)：通过指定关键词搜索主流资讯社交网站中的相关内容，\n\n## 技术文章\n\n- [bert单语种模型训练与部署](https://zhousanfu.github.io/post/nlp-bert-mo-xing-xun-lian-zhi-bu-shu/)\n- [GithubPages与Gitalk 的总结与异常](https://zhousanfu.github.io/post/gitalk-de-zong-jie-yu-yi-chang/)\n\n## 演讲和讲义\n\n- hive核心与实际运用：[内容地址](https://github.com/zhousanfu/zhousanfu.github.io/blob/main/images/hive%E6%A0%B8%E5%BF%83%E4%B8%8E%E5%AE%9E%E9%99%85%E8%BF%90%E7%94%A8.jpg?raw=true)\n\n---\n\n# **技能清单**\n\n- 编程语言：Java、Python、Javascript、Html\n- 框架：Django、Flask、Tensorflow、Scrap\n- 操作系统与数据库：Linux、Mysql、Oracle、Hive、Hadoop、spark、BI\n- 版本管理、自动化部署工具：Git、Conda、Docker",
      "data": {
        "title": "简历",
        "date": "2021-05-21 12:14:35",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "xx"
    },
    {
      "content": "**一、DFA 算法简介**\n\n在实现文字过滤的算法中，DFA是唯一比较好的实现算法。\n\nDFA 全称为：Deterministic Finite Automaton，即确定有穷自动机。其特征为：有一个有限状态集合和一些从一个状态通向另一个状态的边，每条边上标记有一个符号，其中一个状态是初态，某些状态是终态。但不同于不确定的有限自动机，DFA 中不会有从同一状态出发的两条边标志有相同的符号。\n\n![https://nos.netease.com/yidun/5c517db5-d268-455e-a98f-338b93d5ed1e.png](https://nos.netease.com/yidun/5c517db5-d268-455e-a98f-338b93d5ed1e.png)\n\n简单点说就是，它是是通过 event 和当前的 state 得到下一个 state，即 event + state= nextstate。理解为系统中有多个节点，通过传递进入的 event，来确定走哪个路由至另一个节点，而节点是有限的。\n\n**二、DFA 算法实践敏感词过滤**\n\n**1. 敏感词库构造**\n\n以王八蛋和王八羔子两个敏感词来进行描述，首先构建敏感词库，该词库名称为SensitiveMap，这两个词的二叉树构造为：\n\n![https://nos.netease.com/yidun/c30653b6-8728-432e-aeae-e95ed54ed8d6.png](https://nos.netease.com/yidun/c30653b6-8728-432e-aeae-e95ed54ed8d6.png)\n\n用 hash 表构造为：\n\n![https://nos.netease.com/yidun/4fcfd09b-7457-4eca-b4f6-bc1ea509065f.png](https://nos.netease.com/yidun/4fcfd09b-7457-4eca-b4f6-bc1ea509065f.png)\n\n怎么用代码实现这种数据结构呢？\n\n![https://nos.netease.com/yidun/33980271-6c5f-4cfd-ab85-5f150555ecf9.png](https://nos.netease.com/yidun/33980271-6c5f-4cfd-ab85-5f150555ecf9.png)\n\n**2. 敏感词过滤**\n\n以上面例子构造出来的 SensitiveMap 为敏感词库进行示意，假设这里输入的关键字为：王八不好，流程图如下：\n\n![https://nos.netease.com/yidun/8d85693e-e6c5-43ca-91ba-bc9e34e6577a.jpg](https://nos.netease.com/yidun/8d85693e-e6c5-43ca-91ba-bc9e34e6577a.jpg)\n\n怎么用代码实现这个流程图逻辑呢？\n\n![https://nos.netease.com/yidun/cec0241a-8fec-46b1-aca5-a616b2a98cd5.png](https://nos.netease.com/yidun/cec0241a-8fec-46b1-aca5-a616b2a98cd5.png)\n\n**三、优化思路**\n\n对于“王*八&&蛋”这样的词，中间填充了无意义的字符来混淆，在我们做敏感词搜索时，同样应该做一个无意义词的过滤，当循环到这类无意义的字符时进行跳过，避免干扰。\n\n来源：博客园   作者：JMCui\n\n原文链接：https://www.cnblogs.com/jmcui/p/11925777.html\n\n【声明】文章来源于网上采集整理，版权归原作者所有，如有侵权，请邮件反馈yidunmarket@126.com，我们将尽快核实修改。\n\n```python\n# -*- coding:utf-8 -*-\n\nimport time\ntime1=time.time()\n\n# DFA算法\nclass DFAFilter():\n    def __init__(self):\n        self.keyword_chains = {}\n        self.delimit = '\\x00'\n\n    def add(self, keyword):\n        keyword = keyword.lower()\n        chars = keyword.strip()\n        if not chars:\n            return\n        level = self.keyword_chains\n        for i in range(len(chars)):\n            if chars[i] in level:\n                level = level[chars[i]]\n            else:\n                if not isinstance(level, dict):\n                    break\n                for j in range(i, len(chars)):\n                    level[chars[j]] = {}\n                    last_level, last_char = level, chars[j]\n                    level = level[chars[j]]\n                last_level[last_char] = {self.delimit: 0}\n                break\n        if i == len(chars) - 1:\n            level[self.delimit] = 0\n\n    def parse(self, path):\n        with open(path,encoding='utf-8') as f:\n            for keyword in f:\n                self.add(str(keyword).strip())\n\n    def filter(self, message, repl=\"*\"):\n        message = message.lower()\n        ret = []\n        start = 0\n        while start < len(message):\n            level = self.keyword_chains\n            step_ins = 0\n            for char in message[start:]:\n                if char in level:\n                    step_ins += 1\n                    if self.delimit not in level[char]:\n                        level = level[char]\n                    else:\n                        ret.append(repl * step_ins)\n                        start += step_ins - 1\n                        break\n                else:\n                    ret.append(message[start])\n                    break\n            else:\n                ret.append(message[start])\n            start += 1\n\n        return ''.join(ret)\n\nif __name__ == \"__main__\":\n    gfw = DFAFilter()\n    path=\"F:/文本反垃圾算法/sensitive_words.txt\"\n    gfw.parse(path)\n    text=\"新疆骚乱苹果新品发布会雞八\"\n    result = gfw.filter(text)\n\n    print(text)\n    print(result)\n    time2 = time.time()\n    print('总共耗时：' + str(time2 - time1) + 's')\n```\n\n测试结果:\n\n1) 敏感词 100个\n\n- ---------------dfa-----------**message*** 2240.325479984283-----------normal--------------**message*** 224The count of word: 1000.107350111008\n\n2) 敏感词 1000 个\n\n- ---------------dfa-----------**message*** 2240.324251890182-----------normal--------------**message*** 224The count of word: 10001.05939006805\n\n从上面的实验我们可以看出，在DFA 算法只有在敏感词较多的情况下，才有意义。在百来个敏感词的情况下，甚至不如普通算法\n\n下面从理论上推导时间复杂度，为了方便分析，首先假定消息文本是等长的,长度为lenA；每个敏感词的长度相同，长度为lenB，敏感词的个数是m。\n\n1) DFA算法的核心是构建一棵多叉树，由于我们已经假设，敏感词的长度相同，所以树的最大深度为lenB，那么我们可以说从消息文本的某个位置（字节)开始的某个子串是否在敏感词树中，最多只用经过lenB次匹配.也就是说判断一个消息文本中是否有敏感词的时间复杂度是lenA * lenB\n\n2) 再来看看普通做法，是使用for循环，对每一个敏感词，依次在消息文本中进行查找，假定字符串是使用KMP算法,KMP算法的时间复杂度是O(lenA + lenB)\n\n那么对m个敏感词查找的时间复杂度是 (lenA + lenB ) * m\n\n综上所述，DFA 算法的时间复杂度基本上是与敏感词的个数无关的。",
      "data": {
        "title": "DFA 算法实现敏感词过滤",
        "date": "2021-05-07 18:04:24",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "dfa-suan-fa-shi-xian-min-gan-ci-guo-lu"
    },
    {
      "content": "# aircrack-ng(mac)\n\n### 1.install\n\n```bash\nbrew install aircrack-ng\n# 可能需要 安装macport， 安装Xcode、依赖\n# brew install autoconf automake libtool openssl shtool pkg-config hwloc pcre sqlite3 libpcap cmocka\n```\n\n### 2.ifconfig命令查看网卡信息\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/63c78cd1-3e8f-421b-b37b-01dcf6d4124f/t2.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/63c78cd1-3e8f-421b-b37b-01dcf6d4124f/t2.png)\n\n### 3.airport查看网络\n\n```bash\nsudo /System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources/airport -s\n```\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/52134e93-5b62-46ba-8c7d-003aecb23a3f/t1.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/52134e93-5b62-46ba-8c7d-003aecb23a3f/t1.png)\n\n### 4.抓包\nen0默认网卡，6监听频道\n\n```bash\nsudo /System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources/airport en0 sniff 6\n```\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/636dc95e-02d1-453e-bc9d-db4960c36c96/t3.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/636dc95e-02d1-453e-bc9d-db4960c36c96/t3.png)\n\n### 5.破解密码\n\n当执行以上命令， 开始监听以后， **wifi**的图标会**发生改变**， 变成一个小眼睛一样的图标。监听久一点， 然后使用**ctrl+c**停止监听， 系统会把监听到的数据保存到本地， 如下图， 数据保存到**/tmp/airportSniffdaMCjH.cap** 文件中\n\n### 6.查看cap文件\n\n```bash\nsudo aircrack-ng   /tmp/airportSniff8g0Oex.cap\n```\n\n![https://images2015.cnblogs.com/blog/497865/201701/497865-20170108231500300-323707699.png](https://images2015.cnblogs.com/blog/497865/201701/497865-20170108231500300-323707699.png)\n\n如果要查询的路由列表的Encryption值为WPA(1 handshake) ，说明抓取成功， 否者跳到第六步，要重新抓取\n\n### 7.air-crack开始破解\n\n```bash\nsudo aircrack-ng -w password.txt -b 30:91:76:9e:2c:69 /tmp/airportSniffdaMCjH.cap\n# -w：指定字典文件；－b：指定要破解的wifi BSSID。\n#-b后面的参数bc:46:99:df:6c:72指的是网卡的BSSID， 最后面的一个文件路径是上一步监听到的数据\n```\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a632b3c8-a745-484a-a04a-4ec7d147c5c0/t6.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a632b3c8-a745-484a-a04a-4ec7d147c5c0/t6.png)\n\n### 8.字典爆破\n\n有些第三方的网站提供免费爆破，或者收费的爆破， [https://gpuhash.me/](https://gpuhash.me/)\n\n### 9.相关原理\n\n- **WPA/WPA2简介**\n\n由于WEP中存在严重的安全漏洞，WIFI联盟制定了WPA和WPA2以取代WEP。其中WPA实现了802.11i的主要部分，提供了对现有硬件的向下兼容，被用来作为WEP到802.11i的过渡。之后的则WPA2完整的实现了整个IEEE 802.1i标准。WPA的根据应用场景的不同采用不同的认证方式，其中面对家庭或小型办公场所网络的WPA-PSK不需要专门的认证服务器，所有该网络中的设备通过使用同一个256-bit的密钥来进行认证。\n\n- **WPA-PSK安全漏洞**\n\nWPA-PSK认证中的四次握手被设计用来在不安全的信道中，通过明文传输的方式来进行一定程度上的认证，并且在设备之间建立安全信道。首先，**PSK会被转化为PMK**，**而PMK则在接下来被用于生成PTK**。PTK则会被分为若干部分，**其中一部分被称作MIC Key**，用来生成每一个包的Hash值来用于验证。WPA的安全问题与其认证过程所使用的算法关系不大，更多的是由于这一过程可以被轻易的重现，这就使得WPA-PSK可能遭受字典暴力攻击。\n\n- **WPA-PSK攻击原理**\n\nWPA-PSK攻击分为以下几个步骤：　　\n\n1. 根据passphrase，SSID生成PMK，即PMK = pdkdf2_SHA1(passphrase, SSID, SSID length, 4096)　　\n\n2. 捕获EAPOL四次握手的数据包，得到ANonce，SNonce等信息，用于计算PTK，即　　PTK = PRF-X(PMK, Len(PMK), “Pairwise key expansion”, Min(AA,SA) || Max(AA,SA) || Min(ANonce, SNonce) || Max(ANonce, SNonce))　　\n\n3. 使用MIC Key计算EAPOL报文的MIC，即MIC = HMAC_MD5(MIC Key, 16, 802.1x data)　　\n\n4. 将计算得到的MIC值与捕获到的MIC值对比，如果相同则破解成功。\n\n- **WPA-PSK攻击难点**\n\nWPA-PSK攻击的主要难点在于大量计算PMK所需要的计算量。一台普通的计算机通常的计算能力在500pmks/s，想要对8位的纯小写字母组合密码进行暴力破解所需要的时间为14年，所以想要破解WPA－PSK只有两种可能：1.用户使用了常见的弱密码；2.堆砌计算资源，获得超级计算机量级的计算能力。\n\n# **aircrack-ng(kali)**\n\n**使用工具：\n aircrack-ng\n kali支持的无线网卡**\n\n第一步：检查无线网卡插上后，是否识别\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/28efabd8-5c25-47ac-8fed-7b1cde5babf6/20201011093354261.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/28efabd8-5c25-47ac-8fed-7b1cde5babf6/20201011093354261.png)\n\n第二步：airmon-ng check kill (我的理解是杀死有可能妨碍监听模式的进程)\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e9c6bf34-6dbd-4a15-a0c9-d3421c7d6f51/20201011093435169.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e9c6bf34-6dbd-4a15-a0c9-d3421c7d6f51/20201011093435169.png)\n\n第三步：airmon-ng start wlan0 把无线网卡设置为监听模式\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b1bf3505-8fd4-4f8f-80f8-c3ae155934d3/20201011093554376.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b1bf3505-8fd4-4f8f-80f8-c3ae155934d3/20201011093554376.png)\n\n第四步：airodump-ng wlan0 开启监听,确认目标，我们这里以DESKTOP-为例，\n CH表示的工作的信道\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2e9991fe-dfa2-42a0-bc31-b443abf1d145/20201011093700839.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2e9991fe-dfa2-42a0-bc31-b443abf1d145/20201011093700839.png)\n\n第五步：对目标进行抓握手包，只要有设备重新连上这个WIFI，我们就能抓到握手包，但是一直等也不是办法，所以有了第六步\n\n```\nairodump-ng -c 1 -w ret --bssid A2:A4:C5:31:73:E7 wlan0mon\n```\n\n- c 1 表示信道1也就是目标的工作信道 -w ret 表示我们的结果将会存在ret文件里 –bssid A2:A4:C5:31:73:E7 表示目标的MAC地址 wlan0mon 表示监听的网卡\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a70cacf9-52c1-4e85-b09d-10d61cef9128/20201011094421850.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a70cacf9-52c1-4e85-b09d-10d61cef9128/20201011094421850.png)\n\n第六步：强制使设备重连WIFI\n\n```\naireplay-ng -0 10 -a A2:A4:C5:31:73:E7 -c CA:D0:15:1B:A0:F6 wlan0mon\n```\n\n10 表示发10个攻击包\n -a 表示WIFI的MAC地址\n -c 表示设备的MAC地址\n 我们随便选一个把它强制断开WIFI，然后它会再自动连接这个WIFI\n 我们就能拿到握手包\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3820ec7f-817c-4c8e-85c9-6d21dc1e762d/20201011095047535.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3820ec7f-817c-4c8e-85c9-6d21dc1e762d/20201011095047535.png)\n\n这是我们拿到握手包后的截图\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f4d467c0-7953-45d1-b89c-e098bef8f6d6/20201011095352489.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f4d467c0-7953-45d1-b89c-e098bef8f6d6/20201011095352489.png)\n\n第七步：开始暴力破解啦\n\n```\naircrack-ng -w pwd.txt ret-01.cap\n\n```\n\n- w 表示指定的字典文件 ret-01.cap 是第五步我们指定的抓到的包存放的文件名**这个破解的速度还是非常可以的！！！**\n\n![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5e99be77-9d3d-43f6-8af3-c5756bc0ed4c/20201011095943754.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5e99be77-9d3d-43f6-8af3-c5756bc0ed4c/20201011095943754.png)\n\n### 暴力破解概述\n\n本文链接：[https://freeerror.org/d/161-kali-linux-aircrack-ng-wifi](https://freeerror.org/d/161-kali-linux-aircrack-ng-wifi)\n\n穷举法是一种针对于密码的破译方法。这种方法很像数学上的“完全归纳法”并在密码破译方面得到了广泛的应用。简单来说就是将密码进行逐个推算直到找出真正的密码为止。比如一个四位并且全部由数字组成其密码共有10000种组合，也就是说最多我们会尝试9999次才能找到真正的密码。利用这种方法我们可以运用计算机来进行逐个推算，也就是说用我们破解任何一个密码也都只是一个时间问题\n\n当然如果破译一个有8位而且有可能拥有大小写字母、数字、以及符号的密码用普通的家用电脑可能会用掉几个月甚至更多的时间去计算，其组合方法可能有几千万亿种组合。这样长的时间显然是不能接受的。其解决办法就是运用字典，所谓“字典”就是给密码锁定某个范围，比如英文单词以及生日的数字组合等，所有的英文单词不过10万个左右这样可以大大缩小密码范围，很大程度上缩短了破译时间\n\n破解wifi密码操作步骤\n\n需要最少两个终端来实现，以下分别称之为shell 1 和shell 2\n\nShell 1 通过aircrack-ng 工具，将网卡改为监听模式\n\nShell 1 确定目标WiFi 的信息，比如mac 地址和信道，连接数等等\n\nShell 2 模拟无线，抓取密码信息\n\nShell 1 确定目标用户，对其发动攻击\n\nShell 2 得到加密的无线信息并进行破解(通过密码字典) 步骤就是这样了，接下来我来破解下自己的WiFi\n\nWiFi密码破解步骤演示\n\n**开启无线网卡的监听模式，电脑内置的或者外置的都可以** \n\n```\nroot@kali:~# airmon-ng start wlan0\n```\n\n这里要注意的，在开启监听模式之后，wlan0 这个网卡名称现在叫wlan0mon(偶尔也会不变，具体叫什么看上图的提示)\n\n**扫描目标WiFi** \n\n```\nroot@kali:~# airodump-ng wlan0mon\n```\n\n注意现在的连个方框（红色和蓝色区域），现在我们要确认一些信息，及目标AP（就是WiFi，以下简称AP） 的MAC 地址，AP 的信道和加密方式，还有目标用户的MAC地址，我们稍微整理一下： 蓝色区域：目标AP的MAC地址（WiFi路由器的） 红色区域：目标用户的MAC地址（我的手机的） CH（信道）：1 加密方式：WPA2 我们只需要这些信息就足够了\n\n**模拟WiFi 信号** \n\n```\nroot@kali:~# airodump-ng --ivs -w wifi-pass --bssid 1C:60:DE:77:B9:C0 -c 1 wlan0mon\n```\n\n–ivs ：指定生成文件的格式，这里格式是ivs（比如：abc.ivs） -w ：指定文件的名称叫什么，这里叫wifi-pass –bssid ：目标AP的MAC地址，就是之前蓝色区域的 -c ：指定我们模拟的WiFi的信道，这里是1 敲下回车后会看到这样的一段信息，这就说明我们模拟的WiFi 已经开始抓取指定文件了，不过要注意红色箭头的位置，如果想这样一直是空的就是没有抓到需要的信息，如果抓到了看下图，可以对比出来\n\n**攻击指定的用户** 这里使用另一个空闲的终端，执行以下命令 \n\n```\nroot@kali:~# aireplay-ng -0 20 -a 1C:60:DE:77:B9:C0 -c 18:E2:9F:B0:8B:37 wlan0mon\n```\n\n-0 ：发送工具数据包的数量，这里是20个 -a ：指定目标AP的MAC地址 -c ：指定用户的MAC地址，（正在使用WiFi的我的手机） 攻击开始后就像这样～\n\n**得到密码文件并破解**  注意红色箭头指向的位置，如果在发送攻击数据包之后出现了图片里的信息，那么就是密码信息抓取成功了，如果出现了这个的话就可以结束WiFi 模拟了，我们可以按Ctrl+C 然后查看当前目录会发现多了一个wifi-pass-01.ivs 文件，我们想要的密码就在这个文件里，不过是加密的，所有我们还需要通过密码字典把密码破解出来\n\n**指定密码本来破解此文件** \n\n```\nroot@kali:~# aircrack-ng wifi-pass-01.ivs -w /root/pass-heji.txt\n```\n\n-w ： 指定密码字典（比如我的在/root下，所有多了绝对路径） 这里看到红色箭头的位置就是密码了，到这里密码破解就完成了~",
      "data": {
        "title": "wifi密码破解",
        "date": "2021-05-07 18:02:17",
        "tags": [
          "网络攻防"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "wifi-mi-ma-po-jie"
    },
    {
      "content": "特征选择是从原始特征中选择出一些最有效特征以降低数据集维度、提高法性能的方法。\n\n我们知道模型的性能会随着使用特征数量的增加而增加。但是，当超过峰值时，模型性能将会下降。这就是为什么我们只需要选择能够有效预测的特征的原因。\n\n特征选择类似于降维技术，其目的是减少特征的数量，但是从根本上说，它们是不同的。区别在于要素选择会选择要保留或从数据集中删除的要素，而降维会创建数据的投影，从而产生全新的输入要素。\n\n特征选择有很多方法，在本文中我将介绍 Scikit-Learn 中 5 个方法，因为它们是最简单但却非常有用的，让我们开始吧。\n\n### **1、方差阈值特征选择**\n\n具有较高方差的特征表示该特征内的值变化大，较低的方差意味着要素内的值相似，而零方差意味着您具有相同值的要素。\n\n方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征，使用方法我们举例说明：\n\n```\nimport pandas as pd\nimport seaborn as sns\nmpg = sns.load_dataset('mpg').select_dtypes('number')\nmpg.head()\n\n```\n\n\n对于此示例，我仅出于简化目的使用数字特征。在使用方差阈值特征选择之前，我们需要对所有这些数字特征进行转换，因为方差受数字刻度的影响。\n\n```\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nmpg = pd.DataFrame(scaler.fit_transform(mpg), columns = mpg.columns)\nmpg.head()\n\n```\n\n\n所有特征都在同一比例上，让我们尝试仅使用方差阈值方法选择我们想要的特征。假设我的方差限制为一个方差。\n\n```\nfrom sklearn.feature_selection import VarianceThreshold\nselector = VarianceThreshold(1)\nselector.fit(mpg)\nmpg.columns[selector.get_support()]\n\n```\n\n方差阈值是一种无监督学习的特征选择方法。如果我们希望出于监督学习的目的而选择功能怎么办？那就是我们接下来要讨论的。\n\n### **2、SelectKBest特征特征**\n\n单变量特征选择是一种基于单变量统计检验的方法，例如：chi2，Pearson等等。\n\nSelectKBest 的前提是将未经验证的统计测试与基于 X 和 y 之间的统计结果选择 K 数的特征相结合。\n\n```\nmpg = sns.load_dataset('mpg')\nmpg = mpg.select_dtypes('number').dropna()\n#Divide the features into Independent and Dependent Variable\nX = mpg.drop('mpg' , axis =1)\ny = mpg['mpg']\n\n```\n\n由于单变量特征选择方法旨在进行监督学习，因此我们将特征分为独立变量和因变量。接下来，我们将使用SelectKBest，假设我只想要最重要的两个特征。\n\n```\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\n#Select top 2 features based on mutual info regression\nselector = SelectKBest(mutual_info_regression, k =2)\nselector.fit(X, y)\nX.columns[selector.get_support()]\n\n```\n\n### **3、递归特征消除(RFE)**\n\n递归特征消除或RFE是一种特征选择方法，利用机器学习模型通过在递归训练后消除最不重要的特征来选择特征。\n\n根据Scikit-Learn，RFE是一种通过递归考虑越来越少的特征集来选择特征的方法。\n\n- 首先对估计器进行初始特征集训练，然后通过coef_attribute或feature_importances_attribute获得每个特征的重要性。\n- 然后从当前特征中删除最不重要的特征。在修剪后的数据集上递归地重复该过程，直到最终达到所需的要选择的特征数量。\n\n在此示例中，我想使用泰坦尼克号数据集进行分类问题，在那里我想预测谁将生存下来。\n\n```\n#Load the dataset and only selecting the numerical features for example purposes\ntitanic = sns.load_dataset('titanic')[['survived', 'pclass', 'age', 'parch', 'sibsp', 'fare']].dropna()\nX = titanic.drop('survived', axis = 1)\ny = titanic['survived']\n\n```\n\n我想看看哪些特征最能帮助我预测谁可以幸免于泰坦尼克号事件。让我们使用LogisticRegression模型获得最佳特征。\n\n```\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n# #Selecting the Best important features according to Logistic Regression\nrfe_selector = RFE(estimator=LogisticRegression(),n_features_to_select = 2, step = 1)\nrfe_selector.fit(X, y)\nX.columns[rfe_selector.get_support()]\n\n```\n\n默认情况下，为RFE选择的特征数是全部特征的中位数，步长是1.当然，你可以根据自己的经验进行更改。\n\n### **4、SelectFromModel 特征选择**\n\nScikit-Learn 的 SelectFromModel 用于选择特征的机器学习模型估计，它基于重要性属性阈值。默认情况下，阈值是平均值。\n\n让我们使用一个数据集示例来更好地理解这一概念。我将使用之前的数据。\n\n```\nfrom sklearn.feature_selection import SelectFromModel\nsfm_selector = SelectFromModel(estimator=LogisticRegression())\nsfm_selector.fit(X, y)\nX.columns[sfm_selector.get_support()]\n\n```\n\n与RFE一样，你可以使用任何机器学习模型来选择功能，只要可以调用它来估计特征重要性即可。你可以使用随机森林模或XGBoost进行尝试。\n\n### **5、顺序特征选择(SFS)**\n\n顺序特征选择是一种贪婪算法，用于根据交叉验证得分和估计量来向前或向后查找最佳特征，它是 Scikit-Learn 版本0.24中的新增功能。方法如下：\n\n- SFS-Forward 通过从零个特征开始进行功能选择，并找到了一个针对单个特征训练机器学习模型时可以最大化交叉验证得分的特征。\n- 一旦选择了第一个功能，便会通过向所选功能添加新功能来重复该过程。当我们发现达到所需数量的功能时，该过程将停止。\n\n让我们举一个例子说明。\n\n```\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\nsfs_selector = SequentialFeatureSelector(estimator=LogisticRegression(), n_features_to_select = 3, cv =10, direction ='backward')\nsfs_selector.fit(X, y)\nX.columns[sfs_selector.get_support()]\n\n```\n\n### **结论**\n\n特征选择是机器学习模型中的一个重要方面，对于模型无用的特征，不仅影响模型的训练速度，同时也会影响模型的效果。",
      "data": {
        "title": "特征工程的5 种特征选择的方法",
        "date": "2021-04-07 11:53:05",
        "tags": [
          "数据挖掘"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "te-zheng-gong-cheng-de-5-chong-te-zheng-xuan-ze-de-fang-fa"
    },
    {
      "content": "省去环境配置过程\n\n[Google Colaboratory](https://colab.research.google.com/drive/12cnLVS4Ye_dIfEcuI0fQqWwzPeJzLRaB#scrollTo=iyruKALOIQzj)",
      "data": {
        "title": "colab matplotlib 快速使用",
        "date": "2021-03-16 12:46:55",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "colab-matplotlib-kuai-su-shi-yong"
    },
    {
      "content": "[zhousanfu/Tensorflow_Demo](https://github.com/zhousanfu/Tensorflow_Demo)\n\n一、安装\n\n- [Mac M1芯片安装教程](https://towardsdatascience.com/tensorflow-2-4-on-apple-silicon-m1-installation-under-conda-environment-ba6de962b3b8)\n\n```sql\n# M1芯片配适版-Miniconda\nwget -c \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\"\nsh Miniforge3-MacOSX-arm64.sh\n\n# Install specific pip version and some other base packages\npip install --force pip==20.2.4 wheel setuptools cached-property six\n# Install all the packages provided by Apple but TensorFlow\npip install --upgrade --no-dependencies --force numpy-1.18.5-cp38-cp38-macosx_11_0_arm64.whl grpcio-1.33.2-cp38-cp38-macosx_11_0_arm64.whl h5py-2.10.0-cp38-cp38-macosx_11_0_arm64.whl tensorflow_addons-0.11.2+mlcompute-cp38-cp38-macosx_11_0_arm64.whl\n# Install additional packages\npip install absl-py astunparse flatbuffers gast google_pasta keras_preprocessing opt_einsum protobuf tensorflow_estimator termcolor typing_extensions wrapt wheel tensorboard typeguard\n# Install TensorFlow\npip install --upgrade --force --no-dependencies tensorflow_macos-0.1a1-cp38-cp38-macosx_11_0_arm64.whl\n```",
      "data": {
        "title": "Tensorflow2 核心基础",
        "date": "2021-02-09 11:28:07",
        "tags": [
          "Tensorflow"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "tensorflow-ji-chu-he-xin"
    },
    {
      "content": "# **bert单语种模型训练与部署**\n\nTag: bert训练与部署Flask API接口\n\n> bert训练与部署Flask API接口;bert在单语种的准确率较高，但训练与使用占用资源过大，可考虑跨语言模型：XLM\n\n# **一、文件说明**\n\n配置文件(bert_config.json)：用于指定模型的超参数 词典文件(vocab.txt)：用于WordPiece 到 Word id的映射 Tensorflow checkpoint（bert_model.ckpt）：包含了预训练模型的权重（实际包含三个文件）\n\n# **二、BERT结构**\n\n第一阶段：Pre-training，利用无标记的语料训练一个语言模型； 第二阶段：Fine-tuning, 利用预训练好的语言模型，完成具体的NLP下游任务。（run_classifier.py和run_squad.py） extract_features.py-提取特征向量的\n\n# **三、运行**\n\n## **3.1训练：**\n\nrun_classifier.py 解说[[https://blog.csdn.net/weixin_41845265/article/details/107071939](https://blog.csdn.net/weixin_41845265/article/details/107071939)] 最全详细：[[https://blog.csdn.net/weixin_43320501/article/details/93894946?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control](https://blog.csdn.net/weixin_43320501/article/details/93894946?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control)]\n\n```\ncheckpoint 记录可用的模型信息\neval_results.txt 验证集的结果信息\neval.tf_record 记录验证集的二进制信息\nevents.out.tfevents.1590500393.instance-py09166k 用于tensorboard查看详细信息\ngraph.pbtxt 记录tensorflow的结构信息\nlabel2id.pkl 标签信息 （额外加的）\nmodel.ckpt-0* 这里是记录最近的三个文件\nmodel.ckpt-2250.data 所有变量的值\nmodel.ckpt-2250.index 可能是用于映射图和权重关系，0.11版本后引入\nmodel.ckpt-2250.meta 记录完整的计算图结构\npredict.tf_record 预测的二进制文件\ntest_results.tsv 使用预测后生成的预测结果\n\n```\n\n```\nexport BERT_BASE_DIR=./chinese_L-12_H-768_A-12#这里是存放中文模型的路径\nexport DATA_DIR=./data  #这里是存放数据的路径\n\npython3 run_classifier.py \\\n--task_name=my \\     #这里是processor的名字\n--do_train=true \\    #是否训练\n--do_eval=true  \\    #是否验证\n--do_predict=false \\  #是否预测（对应test）\n--do_lower_case=false \\\n--data_dir=$DATA_DIR \\\n--vocab_file=$BERT_BASE_DIR/vocab.txt \\\n--bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n--init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n--max_seq_length=512 \\最大文本程度，最大512\n--train_batch_size=4 \\\n--learning_rate=2e-5 \\\n--num_train_epochs=15 \\\n--output_dir=./mymodel #输出目录\n\n```\n\n## **3.2模型压缩**\n\n> 运行后会在输出文件夹中多出一个 classification_model.pb 文件, 就是压缩后的模型\n\n```\npython freeze_graph.py \\\n    -bert_model_dir=\"bert预训练模型地址\" \\\n    -model_dir=\"模型输出地址(和上边模型训练输出地址一样即可)\" \\\n    -max_seq_len=128 \\  # 序列长度, 需要与训练时 max_seq_length 参书相同\n    -num_labels=3  # label数量\n\n```\n\n## **3.3服务部署(bert-base)**\n\n```\npip install bert-base==0.0.7 -i https://pypi.python.org/simple\nbert-base-serving-start \\\n    -model_dir \"训练好的模型路径\" \\\n    -bert_model_dir \"bert预训练模型路径\" \\\n    -model_pb_dir \"classification_model.pb文件路径\" \\\n    -mode CLASS \\  # 模式, 咱们是分类所以用CLASS\n    -max_seq_len 128 \\  # 序列长度与上边保持一致\n    -port 7006 \\  # 端口号, 不要与其他程序冲突\n    -port_out 7007 # 端口号\n\n```\n\n## **3.4部署测试调用**\n\n```\nfrom bert_base.client import BertClient\nstr1=\"我爱北京天安门\"\nstr2 = \"哈哈哈哈\"\nwith BertClient(show_server_config=False, check_version=False, check_length=False, mode=\"CLASS\", port=5575, port_out=5576) as bc:\n    res = bc.encode([str1, str2])\nprint(res)\n[{'pred_label': ['2', '1'], 'score': [0.9999899864196777, 0.9999299049377441]}]\n\n```\n\n## **3.5报错bert_base/server/http.py：**\n\n```\nsudo pip install flask\nsudo pip install flask_compress\nsudo pip install flask_cors\nsudo pip install flask_json\n\n```\n\n##　3.7预测（测试）：\n\n> TRAINED_CLASSIFIER为刚刚训练的输出目录，无需在进一步指定模型模型名称，否则分类结果会不对\n\n```\nexport BERT_BASE_DIR=./chinese_L-12_H-768_A-12\nexport DATA_DIR=./mymodel\nexport ./mymodel\npython3 run_classifier.py \\\n  --task_name=chi \\\n  --do_predict=true \\\n  --data_dir=$DATA_DIR \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$TRAINED_CLASSIFIER \\\n  --max_seq_length=512 \\\n  --output_dir=./mymodel\n\npython3 /home/zhousanfu/bert_classifier/run_classifier.py --task_name=imodis --do_predict=True --do_lower_case=False --data_dir=/data1/zhousanfu/imo_v1 --vocab_file=/data1/zhousanfu/multi_cased_L-12_H-768_A-12/vocab.txt --bert_config_file=/data1/zhousanfu/multi_cased_L-12_H-768_A-12/bert_config.json --init_checkpoint=/data1/zhousanfu/multi_cased_L-12_H-768_A-12/bert_model.ckpt --max_seq_length=128 --output_dir=/data1/zhousanfu/imo_v7\n\n```\n\n## **3.8 另一种服务TF-serving 部署模型[[https://blog.csdn.net/qq_42693848/article/details/107235688](https://blog.csdn.net/qq_42693848/article/details/107235688)]**\n\n- ([https://blog.csdn.net/JerryZhang__/article/details/85107506?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.control](https://blog.csdn.net/JerryZhang__/article/details/85107506?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.control))\n\n> 使用docker下载tfserving镜像\n\n```\ndocker pull tensorflow/serving:2.1.0  # 这里下载的是tf2.1.0的版本，支持tensorflow1.15以上训练出来的模型。\ndocker run\n    -p 8501:8501\n    -p 8500:8500\n    --mount type=bind,source=/my/model/path/m,target=/models/m\n    -e MODEL_NAME=m\n    -t tensorflow/serving:2.1.0\n上面的命令中：\n(1)-p 8501:8501是端口映射，是将容器的8501端口映射到宿主机的8501端口，后面预测的时候使用该端口；\n(2)-e MODEL_NAME=testnet 设置模型名称；\n(3)--mount type=bind,source=/tmp/testnet,target=/models/testnet 是将宿主机的路径/tmp/testnet挂载到容器的/models/testnet下。/tmp/testnet是存放的是上述准备工作中保存的模型文件，‘testnet’是模型名称，包含一个.pb文件和一个variables文件夹，在/tmp/testnet下新建一个以数字命名的文件夹，如100001，并将模型文件放到该文件夹中。容器内部会根据绑定的路径读取模型文件；\n(4)-t tensorflow/serving 根据名称“tensorflow/serving”运行容器；\n\n$ docker run -p 8501:8501 --mount type=bind,source=/tmp/testnet,target=/models/testnet  -e MODEL_NAME=bert_NLP_\n```",
      "data": {
        "title": "BERT 模型文本分类训练与部署",
        "date": "2021-01-26 16:32:39",
        "tags": [
          "NLP"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "nlp-bert-mo-xing-xun-lian-zhi-bu-shu"
    },
    {
      "content": "# **简单使用**\n\n1. 安装软件：brew install 软件名，例：brew install wget\n2. 搜索软件：brew search 软件名，例：brew search wget\n3. 卸载软件：brew uninstall 软件名，例：brew uninstall wget\n4. 更新所有软件：brew update\n5. 更新具体软件：brew upgrade 软件名 ，例：brew upgrade git\n6. 显示已安装软件：brew list\n7. 查看软件信息：brew info／home 软件名 ，例：brew info git ／ brew home git\n8. PS：brew home指令是用浏览器打开官方网页查看软件信息\n9. 查看哪些已安装的程序需要更新： brew outdated\n10. 显示包依赖：brew reps\n11. 显示帮助：brew help\n\n# 二、ARM X86安装与配置\n\n### Arm版安装：\n\n```bash\ncd /opt # 切换到 /opt 目录\nmkdir homebrew # 创建 homebrew 目录\nsudo chown -R $(whoami) /opt/homebrew # 修改目录所属用户\ncurl -L [https://github.com/Homebrew/brew/tarball/master](https://github.com/Homebrew/brew/tarball/master) | tar xz --strip 1 -C homebrew\n```\n\n### 卸载命令\n\n```bash\n/usr/bin/ruby -e \"(surl -fsSL https://...)\"\n```\n\n### intel版的安装：\n\n```bash\n```shell\n\narch -x86_64 /bin/bash -c \"$(curl -fsSL [https://raw.githubusercontent.com/Homebrew/install/master/install.sh](https://raw.githubusercontent.com/Homebrew/install/master/install.sh))\"\n\n```\n```\n\n### 环境变量设置\n\n```bash\n#vim ~/.zshrc 或者 vim ~/.bash_profile\n\n# x86\n$ export PATH=\"/usr/local/bin:$PATH\"\n$ alias abrew='arch -x86_64 /usr/local/bin/brew'\n\n# arm\n$ export PATH=\"/opt/homebrew/bin:$PATH\"\n$ alias brew='/opt/homebrew/bin/brew'\n```\n\n### 换源下载源\n\n```bash\n# Arm 版\n\ncd \"$(brew --repo)\"\ngit remote set-url origin [git://mirrors](git://mirrors).[ustc.edu.cn/brew.git](http://ustc.edu.cn/brew.git)\ncd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\"\ngit remote set-url origin [git://mirrors](git://mirrors).[ustc.edu.cn/homebrew-core.git](http://ustc.edu.cn/homebrew-core.git)\n\n# intel版\ncd \"$(abrew --repo)\"\ngit remote set-url origin [git://mirrors](git://mirrors).[ustc.edu.cn/brew.git](http://ustc.edu.cn/brew.git)\ncd \"$(abrew --repo)/Library/Taps/homebrew/homebrew-core\"\ngit remote set-url origin [git://mirrors](git://mirrors).[ustc.edu.cn/homebrew-core.git](http://ustc.edu.cn/homebrew-core.git)\n\nbash用户：\necho 'export HOMEBREW_BOTTLE_DOMAIN=[https://mirrors](https://mirrors/).[ustc.edu.cn/homebrew-bottles](http://ustc.edu.cn/homebrew-bottles)' >> ~/.bash_profile\nsource ~/.bash_profile\nzsh用户：\necho 'export HOMEBREW_BOTTLE_DOMAIN=[https://mirrors](https://mirrors/).[ustc.edu.cn/homebrew-bottles](http://ustc.edu.cn/homebrew-bottles)' >> ~/.zshrc\nsource ~/.zshrc\n```",
      "data": {
        "title": "Mac Homebrew 安装与使用",
        "date": "2021-01-15 12:09:09",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "mac-homebrew-an-zhuang-yu-shi-yong"
    },
    {
      "content": "- **文本过滤**\n\n    ```python\n    def review_to_words(data):\n        \n        # 正则去除表情\n        emoji_pattern = re.compile(u'[\\U00010000-\\U0010ffff]')\n        data = emoji_pattern.sub(u'', data)\n        \n        # 正则去除标点\n        fuhao_pattern = re.compile(u'\\.*')\n        data = fuhao_pattern.sub(u'', data)\n        \n        # 正则去除数字\n        digit_pattern = re.compile(u'\\d+')\n        data = digit_pattern.sub(u'', data)\n        \n        # 空格拆分词语\n        words = data.lower().split()\n        \n        # 去掉rmSignal\n        meaningful_words = [w for w in words if not w in rmSignal]\n        \n        # 将筛分好的词合成一个字符串，并用空格分开\n        words = \" \".join(meaningful_words)\n        return words\n    ```",
      "data": {
        "title": "数据清洗",
        "date": "2021-01-03 12:17:54",
        "tags": [
          "数据挖掘"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "chang-yong-shu-ju-qing-xi"
    },
    {
      "content": "\n#****git提交文件****\n\n- 添加文件：`git add 文件名git add -A` 一键add\n- 提交文件:`git commit -m \"提交文件时的说明\"`\n- 推送到远程仓库:普通推送: `git push`强行推送: `git push -u origin master -f`\n\n**一、本地操作：**\n\n**1.其它**\n\ngit init：初始化本地库\n\ngit status：查看工作区、暂存区的状态\n\ngit add <file name>：将工作区的“新建/修改”添加到暂存区\n\ngit rm --cached <file name>：移除暂存区的修改\n\ngit commit <file name>：将暂存区的内容提交到本地库\n\ntip：需要再编辑提交日志，比较麻烦，建议用下面带参数的提交方法\n\ngit commit -m \"提交日志\" <file name>：文件从暂存区到本地库\n\n**2.日志**\n\ngit log：查看历史提交\n\ntip：空格向下翻页，b向上翻页，q退出\n\ngit log --pretty=oneline：以漂亮的一行显示，包含全部哈希索引值\n\ngit log --oneline：以简洁的一行显示，包含简洁哈希索引值\n\ngit reflog：以简洁的一行显示，包含简洁哈希索引值，同时显示移动到某个历史版本所需的步数\n\n**3.版本控制**\n\ngit reset --hard 简洁/完整哈希索引值：回到指定哈希值所对应的版本\n\ngit reset --hard HEAD：强制工作区、暂存区、本地库为当前HEAD指针所在的版本\n\ngit reset --hard HEAD^：后退一个版本\n\ntip：一个^表示回退一个版本\n\ngit reset --hard HEAD~1：后退一个版本\n\ntip：波浪线~后面的数字表示后退几个版本\n\n**4.比较差异**\n\ngit diff：比较工作区和暂存区的**所有文件**差异\n\ngit diff <file name>：比较工作区和暂存区的**指定文件**的差异\n\ngit diff HEAD|HEAD^|HEAD~|哈希索引值 <file name>：比较工作区跟本地库的某个版本的**指定文件**的差异\n\n**5.分支操作**\n\ngit branch -v：查看所有分支\n\ngit branch -d <分支名>：删除本地分支\n\ngit branch <分支名>：新建分支\n\ngit checkout <分支名>：切换分支\n\ngit merge <被合并分支名>：合并分支\n\ntip：如master分支合并 hot_fix分支，那么当前必须处于master分支上，然后执行 git merge hot_fix 命令\n\ntip2：合并出现冲突\n\n①删除git自动标记符号，如<<<<<<< HEAD、>>>>>>>等\n\n②修改到满意后，保存退出\n\n③git add <file name>\n\n④git commit -m \"日志信息\"，此时后面不要带文件名\n\n**二、本地库跟远程库交互：**\n\ngit clone <远程库地址>：克隆远程库\n\n功能：①完整的克隆远程库为本地库，②为本地库新建origin别名，③初始化本地库\n\ngit remote -v：查看远程库地址别名\n\ngit remote add <别名> <远程库地址>：新建远程库地址别名\n\ngit remote rm <别名>：删除本地中远程库别名\n\ngit push <别名> <分支名>：本地库某个分支推送到远程库，分支必须指定\n\ngit pull <别名> <分支名>：把远程库的修改拉取到本地\n\ntip：该命令包括git fetch，git merge\n\ngit fetch <远程库别名> <远程库分支名>：抓取远程库的指定分支到本地，但没有合并\n\ngit merge <远程库别名/远程库分支名>：将抓取下来的远程的分支，跟当前所在分支进行合并\n\ngit fork：复制远程库\n\ntip：一般是外面团队的开发人员fork本团队项目，然后进行开发，之后外面团队发起pull request，然后本团队进行审核，如无问题本团队进行merge（合并）到团队自己的远程库，整个流程就是本团队跟外面团队的协同开发流程，Linux的团队开发成员即为这种工作方式。\n\n### 三、创建git\n\n```bash\ngit remote add origin https://github.com/zhousanfu/digital_monitoring_liunx.git\ngit push origin master\n```",
      "data": {
        "title": "Git 使用之命令行篇",
        "date": "2020-02-26 17:06:14",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "git-shi-yong-ming-zhi-ling-xing-pian"
    },
    {
      "content": "Jupyter Notebook 是数据科学 / 机器学习社区内一款非常流行的工具\n\n最近入手了一台新的服务器，于是我想到将 Jupyter 搭建到服务器上，在任何只要有浏览器的地方都能进行 Python 编程\n\n我的服务器是 ubuntu 的，不同系统根据自己系统的命令进行操作\n\n### **创建用户、切换用户展开目录**\n\n首先在 root 用户下打开防火墙 8888 端口，这是提供 Jupyter 服务的端口\n\n```\nsudo ufw allow 8888\n\n```\n\n然后创建一个用户名为 `mathor` 的用户\n\n```\nsudo adduser mathor\n\n```\n\n输入密码并确认密码\n\n然后一路 Enter，默认就行，最后输入 `y` 确认一下\n\n然后切换到新用户，并进入当前用户的主目录\n\n```\nsu mathor\n\ncd ~\n\n```\n\n### **下载并安装 Anaconda展开目录**\n\nAnaconda 的 Linux 下载网址是 **[https://www.anaconda.com/download/#linux](https://www.anaconda.com/download/#linux)**\n\n截止到今天的最新版本是 2018.12，所以通过命令下载\n\n```\nwget https://repo.continuum.io/archive/Anaconda3-2018.12-Linux-x86_64.sh\n\n```\n\n下载完成后运行\n\n```\nbash Anaconda3-2018.12-Linux-x86_64.sh\n\n```\n\n之后会有一个协议，输入 `yes`，然后会有安装路径选择，按下 Enter 就是默认路径，之后会问是否要加入到环境变量，输入 `yes`，之后问要不要安装 vs code，输入 `no`。最后安装完成，输入\n\n```\njupyter\n\n```\n\n按两下 tab 键提示很多东西，就证明通过 Anaconda 安装 Jupyter 成功了，如果没有反应，同时发现输入 `conda` 没有命令，那么执行下面两步就可以了\n\n```\necho 'export PATH=\"~/anaconda3/bin:$PATH\"'>>~/.bashrc\n\nsource ~/.bashrc\n\n```\n\n### **配置 Jupyter展开目录**\n\n运行命令\n\n```\njupyter-notebook --generate-config\n\n```\n\n这时看到一个反馈\n\n```\nWritingdefault config to:/home/mathor/.jupyter/jupyter_notebook_config.py\n\n```\n\n这就是配置的目录。然后运行命令\n\n```\njupyter-notebook password\n\n```\n\n输入密码并确认，这就是以后登陆的密码\n\n输入命令\n\n```\nvi .jupyter/jupyter_notebook_config.json\n\n```\n\n可以看到有一个字符串 `sha1:xxxxxxx`，复制下来，一会要用到。然后运行命令\n\n```\nmkdir jupyterdata\n\n```\n\n创造一个文件夹存放 jupyter 的代码。最后配置端口与代码存放路径\n\n```\nvi .jupyter/jupyter_notebook_config.py\n\n```\n\n随便在空白处写上下面的关键配置即可：\n\n```\n# 设置默认目录\n\nc.NotebookApp.notebook_dir = u'/home/mathor/jupyterdata'\n\n# 允许通过任意绑定服务器的ip访问\n\nc.NotebookApp.ip = '*'\n\n# 用于访问的端口\n\nc.NotebookApp.port = 8888\n\n# 不自动打开浏览器\n\nc.NotebookApp.open_browser = False\n\n# 设置登录密码\n\nc.NotebookApp.password = u'sha1:xxxxxxxxxxxxxxxx'\n\n```\n\n保存并退出（按下 ESC，输入`:wq` 回车）\n\n然后运行\n\n```\njupyter-notebook\n\n```\n\n如果出现下面的报错\n\n```\nPermissionError: [Errno 13] Permission denied: '/run/user/xxxx/jupyter'\n\n```\n\n则输入\n\n```\nexport XDG_RUNTIME_DIR=\"/home/mathor/anaconda3\"\n\nsource .bashrc\n\njupyter-notebook\n\n```\n\n然后就 OK 了\n\n### **本地测试展开目录**\n\n随便在一个客户机浏览器里输入 `http://ip:8888` 就可以进入 Jupyter 的登陆界面了",
      "data": {
        "title": "搭建云端 Jupyter",
        "date": "2020-02-09 12:28:42",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "da-jian-yun-duan-jupyter"
    },
    {
      "content": "### **Byte Pair Encoding**\n\n在 NLP 模型中，输入通常是一个句子，例如 `\"I went to New York last week.\"`，一句话中包含很多单词（token）。传统的做法是将这些单词以空格进行分隔，例如 `['i', 'went', 'to', 'New', 'York', 'last', 'week']`。然而这种做法存在很多问题，例如模型无法通过 `old, older, oldest` 之间的关系学到 `smart, smarter, smartest` 之间的关系。如果我们能使用将一个 token 分成多个 subtokens，上面的问题就能很好的解决。本文将详述目前比较常用的 subtokens 算法 ——BPE（Byte-Pair Encoding）\n\n现在性能比较好一些的 NLP 模型，例如 GPT、BERT、RoBERTa 等，在数据预处理的时候都会有 WordPiece 的过程，其主要的实现方式就是 BPE（Byte-Pair Encoding）。具体来说，例如 `['loved', 'loving', 'loves']` 这三个单词。其实本身的语义都是 \"爱\" 的意思，但是如果我们以词为单位，那它们就算不一样的词，在英语中不同后缀的词非常的多，就会使得词表变的很大，训练速度变慢，训练的效果也不是太好。BPE 算法通过训练，能够把上面的 3 个单词拆分成 `[\"lov\",\"ed\",\"ing\",\"es\"]` 几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。算法流程如下：\n\n1. 设定最大 subwords 个数\n\n    V\n\n2. 将所有单词拆分为单个字符，并在最后添加一个停止符 `</w>`，同时标记出该单词出现的次数。例如，`\"low\"` 这个单词出现了 5 次，那么它将会被处理为 `{'l o w </w>': 5}`\n3. 统计每一个连续**字节对**的出现频率，选择最高频者合并成新的 subword\n4. 重复第 3 步直到达到第 1 步设定的 subwords 词表大小或下一个最高频的**字节对**出现频率为 1\n\n例如\n\n```\n{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n\n```\n\n出现最频繁的字节对是**`e`**和**`s`**，共出现了 6+3=9 次，因此将它们合并\n\n```\n{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n\n```\n\n出现最频繁的字节对是**`es`**和**`t`**，共出现了 6+3=9 次，因此将它们合并\n\n```\n{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n\n```\n\n出现最频繁的字节对是**`est`**和**`</w>`**，共出现了 6+3=9 次，因此将它们合并\n\n```\n{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n\n```\n\n出现最频繁的字节对是**`l`**和**`o`**，共出现了 5+2=7 次，因此将它们合并\n\n```\n{'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n\n```\n\n出现最频繁的字节对是**`lo`**和**`w`**，共出现了 5+2=7 次，因此将它们合并\n\n```\n{'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n\n```\n\n...... 继续迭代直到达到预设的 subwords 词表大小或下一个最高频的字节对出现频率为 1。这样我们就得到了更加合适的词表，这个词表可能会出现一些不是单词的组合，但是其本身有意义的一种形式\n\n停止符 `</w>` 的意义在于表示 subword 是词后缀。举例来说：`st` 不加 `</w>` 可以出现在词首，如 `st ar`；加了 `</w>` 表明改字词位于词尾，如 `wide st</w>`，二者意义截然不同\n\n### **BPE 实现**\n\n```\nimport re, collections\n\ndefget_vocab(filename):\n\n    vocab = collections.defaultdict(int)\n\nwith open(filename, 'r', encoding='utf-8')as fhand:\n\nfor linein fhand:\n\n            words = line.strip().split()\n\nfor wordin words:\n\n                vocab[' '.join(list(word)) + ' </w>'] += 1\n\nreturn vocab\n\ndefget_stats(vocab):\n\n    pairs = collections.defaultdict(int)\n\nfor word, freqin vocab.items():\n\n        symbols = word.split()\n\nfor iin range(len(symbols)-1):\n\n            pairs[symbols[i],symbols[i+1]] += freq\n\nreturn pairs\n\ndefmerge_vocab(pair, v_in):\n\n    v_out = {}\n\n    bigram = re.escape(' '.join(pair))\n\n    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n\nfor wordin v_in:\n\n        w_out = p.sub(''.join(pair), word)\n\n        v_out[w_out] = v_in[word]\n\nreturn v_out\n\ndefget_tokens(vocab):\n\n    tokens = collections.defaultdict(int)\n\nfor word, freqin vocab.items():\n\n        word_tokens = word.split()\n\nfor tokenin word_tokens:\n\n            tokens[token] += freq\n\nreturn tokens\n\nvocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n\n# Get free book from Gutenberg\n\n# wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt\n\n# vocab = get_vocab('pg16457.txt')\n\nprint('==========')\n\nprint('Tokens Before BPE')\n\ntokens = get_tokens(vocab)\n\nprint('Tokens: {}'.format(tokens))\n\nprint('Number of tokens: {}'.format(len(tokens)))\n\nprint('==========')\n\nnum_merges = 5\n\nfor iin range(num_merges):\n\n    pairs = get_stats(vocab)\n\nifnot pairs:\n\nbreak\n    best = max(pairs, key=pairs.get)\n\n    vocab = merge_vocab(best, vocab)\n\n    print('Iter: {}'.format(i))\n\n    print('Best pair: {}'.format(best))\n\n    tokens = get_tokens(vocab)\n\n    print('Tokens: {}'.format(tokens))\n\n    print('Number of tokens: {}'.format(len(tokens)))\n\n    print('==========')\n\n```\n\n输出如下\n\n```\n==========\n\nTokens Before BPE\n\nTokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 16, 'e': 17, 'r': 2, 'n': 6, 's': 9, 't': 9, 'i': 3, 'd': 3})\n\nNumber of tokens: 11\n\n==========\n\nIter: 0\n\nBest pair: ('e', 's')\n\nTokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 16, 'e': 8, 'r': 2, 'n': 6, 'es': 9, 't': 9, 'i': 3, 'd': 3})\n\nNumber of tokens: 11\n\n==========\n\nIter: 1\n\nBest pair: ('es', 't')\n\nTokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 16, 'e': 8, 'r': 2, 'n': 6, 'est': 9, 'i': 3, 'd': 3})\n\nNumber of tokens: 10\n\n==========\n\nIter: 2\n\nBest pair: ('est', '</w>')\n\nTokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'est</w>': 9, 'i': 3, 'd': 3})\n\nNumber of tokens: 10\n\n==========\n\nIter: 3\n\nBest pair: ('l', 'o')\n\nTokens: defaultdict(<class 'int'>, {'lo': 7, 'w': 16, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'est</w>': 9, 'i': 3, 'd': 3})\n\nNumber of tokens: 9\n\n==========\n\nIter: 4\n\nBest pair: ('lo', 'w')\n\nTokens: defaultdict(<class 'int'>, {'low': 7, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'w': 9, 'est</w>': 9, 'i': 3, 'd': 3})\n\nNumber of tokens: 9\n\n==========\n\n```\n\n### **编码和解码**\n\n### **编码**\n\n在之前的算法中，我们已经得到了 subword 的词表，对该词表按照字符个数由多到少排序。编码时，对于每个单词，遍历排好序的子词词表寻找是否有 token 是当前单词的子字符串，如果有，则该 token 是表示单词的 tokens 之一\n\n我们从最长的 token 迭代到最短的 token，尝试将每个单词中的子字符串替换为 token。 最终，我们将迭代所有 tokens，并将所有子字符串替换为 tokens。 如果仍然有子字符串没被替换但所有 token 都已迭代完毕，则将剩余的子词替换为特殊 token，如 `<unk>`\n\n例如\n\n```\n# 给定单词序列\n\n[\"the</w>\", \"highest</w>\", \"mountain</w>\"]\n\n# 排好序的subword表\n\n# 长度 6         5           4        4         4       4          2\n\n[\"errrr</w>\", \"tain</w>\", \"moun\", \"est</w>\", \"high\", \"the</w>\", \"a</w>\"]\n\n# 迭代结果\n\n\"the</w>\" -> [\"the</w>\"]\n\n\"highest</w>\" -> [\"high\", \"est</w>\"]\n\n\"mountain</w>\" -> [\"moun\", \"tain</w>\"]\n\n```\n\n### **解码**\n\n将所有的 tokens 拼在一起即可，例如\n\n```\n# 编码序列\n\n[\"the</w>\", \"high\", \"est</w>\", \"moun\", \"tain</w>\"]\n\n# 解码序列\n\n\"the</w> highest</w> mountain</w>\"\n\n```\n\n### **编码和解码实现**\n\n```\nimport re, collections\n\ndefget_vocab(filename):\n\n    vocab = collections.defaultdict(int)\n\nwith open(filename, 'r', encoding='utf-8')as fhand:\n\nfor linein fhand:\n\n            words = line.strip().split()\n\nfor wordin words:\n\n                vocab[' '.join(list(word)) + ' </w>'] += 1\n\nreturn vocab\n\ndefget_stats(vocab):\n\n    pairs = collections.defaultdict(int)\n\nfor word, freqin vocab.items():\n\n        symbols = word.split()\n\nfor iin range(len(symbols)-1):\n\n            pairs[symbols[i],symbols[i+1]] += freq\n\nreturn pairs\n\ndefmerge_vocab(pair, v_in):\n\n    v_out = {}\n\n    bigram = re.escape(' '.join(pair))\n\n    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n\nfor wordin v_in:\n\n        w_out = p.sub(''.join(pair), word)\n\n        v_out[w_out] = v_in[word]\n\nreturn v_out\n\ndefget_tokens_from_vocab(vocab):\n\n    tokens_frequencies = collections.defaultdict(int)\n\n    vocab_tokenization = {}\n\nfor word, freqin vocab.items():\n\n        word_tokens = word.split()\n\nfor tokenin word_tokens:\n\n            tokens_frequencies[token] += freq\n\n        vocab_tokenization[''.join(word_tokens)] = word_tokens\n\nreturn tokens_frequencies, vocab_tokenization\n\ndefmeasure_token_length(token):\n\nif token[-4:] == '</w>':\n\nreturn len(token[:-4]) + 1\n\nelse:\n\nreturn len(token)\n\ndeftokenize_word(string, sorted_tokens, unknown_token='</u>'):\n\nif string == '':\n\nreturn []\n\nif sorted_tokens == []:\n\nreturn [unknown_token]\n\n    string_tokens = []\n\nfor iin range(len(sorted_tokens)):\n\n        token = sorted_tokens[i]\n\n        token_reg = re.escape(token.replace('.', '[.]'))\n\n        matched_positions = [(m.start(0), m.end(0))for min re.finditer(token_reg, string)]\n\nif len(matched_positions) == 0:\n\ncontinue\n        substring_end_positions = [matched_position[0]for matched_positionin matched_positions]\n\n        substring_start_position = 0\n\nfor substring_end_positionin substring_end_positions:\n\n            substring = string[substring_start_position:substring_end_position]\n\n            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n\n            string_tokens += [token]\n\n            substring_start_position = substring_end_position + len(token)\n\n        remaining_substring = string[substring_start_position:]\n\n        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n\nbreak\nreturn string_tokens\n\n# vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n\nvocab = get_vocab('pg16457.txt')\n\nprint('==========')\n\nprint('Tokens Before BPE')\n\ntokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n\nprint('All tokens: {}'.format(tokens_frequencies.keys()))\n\nprint('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n\nprint('==========')\n\nnum_merges = 10000\n\nfor iin range(num_merges):\n\n    pairs = get_stats(vocab)\n\nifnot pairs:\n\nbreak\n    best = max(pairs, key=pairs.get)\n\n    vocab = merge_vocab(best, vocab)\n\n    print('Iter: {}'.format(i))\n\n    print('Best pair: {}'.format(best))\n\n    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n\n    print('All tokens: {}'.format(tokens_frequencies.keys()))\n\n    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n\n    print('==========')\n\n# Let's check how tokenization will be for a known word\n\nword_given_known = 'mountains</w>'\n\nword_given_unknown = 'Ilikeeatingapples!</w>'\n\nsorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n\nsorted_tokens = [tokenfor (token, freq)in sorted_tokens_tuple]\n\nprint(sorted_tokens)\n\nword_given = word_given_known\n\nprint('Tokenizing word: {}...'.format(word_given))\n\nif word_givenin vocab_tokenization:\n\n    print('Tokenization of the known word:')\n\n    print(vocab_tokenization[word_given])\n\n    print('Tokenization treating the known word as unknown:')\n\n    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n\nelse:\n\n    print('Tokenizating of the unknown word:')\n\n    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n\nword_given = word_given_unknown\n\nprint('Tokenizing word: {}...'.format(word_given))\n\nif word_givenin vocab_tokenization:\n\n    print('Tokenization of the known word:')\n\n    print(vocab_tokenization[word_given])\n\n    print('Tokenization treating the known word as unknown:')\n\n    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n\nelse:\n\n    print('Tokenizating of the unknown word:')\n\n    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n\n```\n\n输出如下\n\n```\nTokenizing word: mountains</w>...\n\nTokenizationof the known word:\n\n['mountains</w>']\n\nTokenization treating the known wordas unknown:\n\n['mountains</w>']\n\nTokenizing word: Ilikeeatingapples!</w>...\n\nTokenizatingof the unknown word:\n\n['I', 'like', 'ea', 'ting', 'app', 'l', 'es!</w>']\n\n```\n\n### **Reference**\n\n- **[3 subword algorithms help to improve your NLP model performance](https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46)**\n- **[Tokenizers: How machines read](https://blog.floydhub.com/tokenization-nlp/)**\n- **[Overview of tokenization algorithms in NLP](https://towardsdatascience.com/overview-of-nlp-tokenization-algorithms-c41a7d5ec4f9)**\n- **[一文读懂 BERT 中的 WordPiece](https://www.cnblogs.com/huangyc/p/10223075.html)**\n- **[Byte Pair Encoding](https://leimao.github.io/blog/Byte-Pair-Encoding/)**\n- **[深入理解 NLP Subword 算法：BPE、WordPiece、ULM](https://zhuanlan.zhihu.com/p/86965595)**",
      "data": {
        "title": "BPE 算法详解",
        "date": "2020-02-03 09:19:23",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "bpe-suan-fa-xiang-jie"
    },
    {
      "content": "一、异常问题\n===============\n## Gitalk-设置后连接403原因：\n![](https://zhousanfu.github.io/post-images/1614321281694.png)\n解决方法：访问-[申请解开封锁](https://cors-anywhere.herokuapp.com/corsdemo)\n返回：You currently have temporary access to the demo server，就可。\n\n--官方文档解释：https://github.com/Rob--W/cors-anywhere/issues/301\n",
      "data": {
        "title": "GithubPages与Gitalk 的总结与异常",
        "date": "2020-01-26 14:13:35",
        "tags": [
          "前端",
          "Gridea"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "gitalk-de-zong-jie-yu-yi-chang"
    },
    {
      "content": "新手友好 [Markdown 入门参考](https://xianbai.me/learn-md/index.html)",
      "data": {
        "title": "Learning-Markdown (Markdown 入门参考)",
        "date": "2019-02-26 16:46:00",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "learning-markdown-markdown-ru-men-can-kao"
    },
    {
      "content": "👏  欢迎使用 **Gridea** ！  \n✍️  **Gridea** 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ... \n\n<!-- more -->\n\n[Github](https://github.com/getgridea/gridea)  \n[Gridea 主页](https://gridea.dev/)  \n[示例网站](http://fehey.com/)\n\n## 特性👇\n📝  你可以使用最酷的 **Markdown** 语法，进行快速创作  \n\n🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片  \n\n🏷️  你可以对文章进行标签分组  \n\n📋  你可以自定义菜单，甚至可以创建外部链接菜单  \n\n💻  你可以在 **Windows**，**MacOS** 或 **Linux** 设备上使用此客户端  \n\n🌎  你可以使用 **𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌** 或 **Coding Pages** 向世界展示，未来将支持更多平台  \n\n💬  你可以进行简单的配置，接入 [Gitalk](https://github.com/gitalk/gitalk) 或 [DisqusJS](https://github.com/SukkaW/DisqusJS) 评论系统  \n\n🇬🇧  你可以使用**中文简体**或**英语**  \n\n🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力  \n\n🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步  \n\n🌱 当然 **Gridea** 还很年轻，有很多不足，但请相信，它会不停向前 🏃\n\n未来，它一定会成为你离不开的伙伴\n\n尽情发挥你的才华吧！\n\n😘 Enjoy~\n",
      "data": {
        "title": "Hello Gridea",
        "date": "2019-02-05 09:05:06",
        "tags": [
          "Gridea"
        ],
        "published": true,
        "hideInList": false,
        "feature": "/post-images/hello-gridea.png",
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "👏  欢迎使用 **Gridea** ！  \n✍️  **Gridea** 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ... ",
      "fileName": "hello-gridea"
    },
    {
      "content": "> 欢迎来到我的小站呀，很高兴遇见你！🤝\n\n## 🏠 关于本站\n学习笔记\n\n## 👨‍💻 博主是谁\n周三甫\n\n## ⛹ 兴趣爱好\n\n## 📬 联系我呀\n\n",
      "data": {
        "title": "关于",
        "date": "2019-01-25 19:09:48",
        "tags": [],
        "published": true,
        "hideInList": true,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "about"
    }
  ],
  "tags": [
    {
      "index": -1,
      "name": "网络攻防",
      "slug": "pS9loTMpi",
      "used": true
    },
    {
      "index": -1,
      "name": "Tensorflow",
      "slug": "Tensorflow",
      "used": true
    },
    {
      "index": -1,
      "name": "数据挖掘",
      "slug": "data-grub",
      "used": true
    },
    {
      "index": -1,
      "name": "NLP",
      "slug": "nlp",
      "used": true
    },
    {
      "index": -1,
      "name": "前端",
      "slug": "front_end",
      "used": true
    },
    {
      "name": "Gridea",
      "slug": "xyBbGRwYX",
      "used": true
    }
  ],
  "menus": [
    {
      "link": "https://zhousanfu.github.io/post/xx",
      "name": "首页",
      "openType": "Internal"
    },
    {
      "link": "/archives",
      "name": "归档",
      "openType": "Internal"
    },
    {
      "link": "/tags",
      "name": "标签",
      "openType": "Internal"
    },
    {
      "link": "/post/about",
      "name": "关于",
      "openType": "Internal"
    }
  ]
}