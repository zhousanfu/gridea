{
  "posts": [
    {
      "content": "çœå»ç¯å¢ƒé…ç½®è¿‡ç¨‹\r\n\r\n[Google Colaboratory](https://colab.research.google.com/drive/12cnLVS4Ye_dIfEcuI0fQqWwzPeJzLRaB#scrollTo=iyruKALOIQzj)",
      "data": {
        "title": "colab matplotlib å¿«é€Ÿä½¿ç”¨",
        "date": "2021-03-16 12:46:55",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "colab-matplotlib-kuai-su-shi-yong"
    },
    {
      "content": "\r\n# ä¸ªäººä¿¡æ¯\r\n\r\n - å‘¨ä¸‰ç”«/ç”·/1998 \r\n - å¤§å­¦ï¼šjavaè½¯ä»¶å¼€å‘ \r\n - å·¥ä½œå¹´é™ï¼š2å¹´\r\n - æŠ€æœ¯åšå®¢ï¼šhttps://zhousanfu.github.io\r\n - Githubï¼šhttps://github.com/zhousanfu (æœ‰åŸåˆ›repo)\r\n\r\n\r\n# å·¥ä½œç»å†\r\nï¼ˆå·¥ä½œç»å†æŒ‰é€†åºæ’åˆ—ï¼Œæœ€æ–°çš„åœ¨æœ€å‰è¾¹ï¼ŒæŒ‰å…¬å¸åšä¸€çº§åˆ†ç»„ï¼Œå…¬å¸å†…æŒ‰äºŒçº§åˆ†ç»„ï¼‰\r\n\r\n## ABCå…¬å¸ ï¼ˆ 2012å¹´9æœˆ ~ 2014å¹´9æœˆ ï¼‰\r\n\r\n### DEFé¡¹ç›® \r\næˆ‘åœ¨æ­¤é¡¹ç›®è´Ÿè´£äº†å“ªäº›å·¥ä½œï¼Œåˆ†åˆ«åœ¨å“ªäº›åœ°æ–¹åšå¾—å‡ºè‰²/å’Œåˆ«äººä¸ä¸€æ ·/æˆé•¿å¿«ï¼Œè¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘æœ€å›°éš¾çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Œæˆ‘é‡‡å–äº†ä»€ä¹ˆæªæ–½ï¼Œæœ€åç»“æœå¦‚ä½•ã€‚è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘æœ€è‡ªè±ªçš„æŠ€æœ¯ç»†èŠ‚æ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆï¼Œå®æ–½å‰å’Œå®æ–½åçš„æ•°æ®å¯¹æ¯”å¦‚ä½•ï¼ŒåŒäº‹å’Œé¢†å¯¼å¯¹æ­¤çš„ååº”å¦‚ä½•ã€‚\r\n\r\n\r\n### GHIé¡¹ç›® \r\næˆ‘åœ¨æ­¤é¡¹ç›®è´Ÿè´£äº†å“ªäº›å·¥ä½œï¼Œåˆ†åˆ«åœ¨å“ªäº›åœ°æ–¹åšå¾—å‡ºè‰²/å’Œåˆ«äººä¸ä¸€æ ·/æˆé•¿å¿«ï¼Œè¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘æœ€å›°éš¾çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Œæˆ‘é‡‡å–äº†ä»€ä¹ˆæªæ–½ï¼Œæœ€åç»“æœå¦‚ä½•ã€‚è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘æœ€è‡ªè±ªçš„æŠ€æœ¯ç»†èŠ‚æ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆï¼Œå®æ–½å‰å’Œå®æ–½åçš„æ•°æ®å¯¹æ¯”å¦‚ä½•ï¼ŒåŒäº‹å’Œé¢†å¯¼å¯¹æ­¤çš„ååº”å¦‚ä½•ã€‚\r\n\r\n\r\n### å…¶ä»–é¡¹ç›®\r\n\r\nï¼ˆæ¯ä¸ªå…¬å¸å†™2~3ä¸ªæ ¸å¿ƒé¡¹ç›®å°±å¥½äº†ï¼Œå¦‚æœä½ æœ‰éå¸¸å¤§é‡çš„é¡¹ç›®ï¼Œé‚£ä¹ˆæŒ‰åˆ†ç±»è¿›è¡Œåˆå¹¶ï¼Œæ¯ä¸€ç±»é€‰ä¸€ä¸ªå…¸å‹å†™å‡ºæ¥ã€‚å…¶ä»–çš„ä¸€ç¬”å¸¦è¿‡å³å¯ã€‚ï¼‰\r\n\r\n  \r\n  \r\n# æŠ€èƒ½æ¸…å•\r\nä»¥ä¸‹å‡ä¸ºæˆ‘ç†Ÿç»ƒä½¿ç”¨çš„æŠ€èƒ½\r\n- æ¡†æ¶ï¼šDjango/Tensorflow\r\n- ç¼–ç¨‹è¯­è¨€ï¼šJava/Python\r\n- ç‰ˆæœ¬ç®¡ç†ï¼šGit\r\n- æ•°æ®åº“ç›¸å…³ï¼šMysql/Oracle/Hive/Hadoop/\r\n- äº‘å’Œå¼€æ”¾å¹³å°ï¼šSAE/BAE/AWS/å¾®ä¿¡åº”ç”¨å¼€å‘\r\n\r\n",
      "data": {
        "title": "XX",
        "date": "2021-03-09 12:14:35",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "xx"
    },
    {
      "content": "[zhousanfu/Tensorflow_Demo](https://github.com/zhousanfu/Tensorflow_Demo)\r\n\r\nä¸€ã€å®‰è£…\r\n\r\n- [Mac M1èŠ¯ç‰‡å®‰è£…æ•™ç¨‹](https://towardsdatascience.com/tensorflow-2-4-on-apple-silicon-m1-installation-under-conda-environment-ba6de962b3b8)\r\n\r\n```sql\r\n# M1èŠ¯ç‰‡é…é€‚ç‰ˆ-Miniconda\r\nwget -c \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\"\r\nsh Miniforge3-MacOSX-arm64.sh\r\n\r\n# Install specific pip version and some other base packages\r\npip install --force pip==20.2.4 wheel setuptools cached-property six\r\n# Install all the packages provided by Apple but TensorFlow\r\npip install --upgrade --no-dependencies --force numpy-1.18.5-cp38-cp38-macosx_11_0_arm64.whl grpcio-1.33.2-cp38-cp38-macosx_11_0_arm64.whl h5py-2.10.0-cp38-cp38-macosx_11_0_arm64.whl tensorflow_addons-0.11.2+mlcompute-cp38-cp38-macosx_11_0_arm64.whl\r\n# Install additional packages\r\npip install absl-py astunparse flatbuffers gast google_pasta keras_preprocessing opt_einsum protobuf tensorflow_estimator termcolor typing_extensions wrapt wheel tensorboard typeguard\r\n# Install TensorFlow\r\npip install --upgrade --force --no-dependencies tensorflow_macos-0.1a1-cp38-cp38-macosx_11_0_arm64.whl\r\n```",
      "data": {
        "title": "Tensorflow2 æ ¸å¿ƒåŸºç¡€",
        "date": "2021-02-09 11:28:07",
        "tags": [
          "Tensorflow"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "tensorflow-ji-chu-he-xin"
    },
    {
      "content": "# **bertå•è¯­ç§æ¨¡å‹è®­ç»ƒä¸éƒ¨ç½²**\r\n\r\nTag: bertè®­ç»ƒä¸éƒ¨ç½²Flask APIæ¥å£\r\n\r\n> bertè®­ç»ƒä¸éƒ¨ç½²Flask APIæ¥å£;bertåœ¨å•è¯­ç§çš„å‡†ç¡®ç‡è¾ƒé«˜ï¼Œä½†è®­ç»ƒä¸ä½¿ç”¨å ç”¨èµ„æºè¿‡å¤§ï¼Œå¯è€ƒè™‘è·¨è¯­è¨€æ¨¡å‹ï¼šXLM\r\n\r\n# **ä¸€ã€æ–‡ä»¶è¯´æ˜**\r\n\r\né…ç½®æ–‡ä»¶(bert_config.json)ï¼šç”¨äºæŒ‡å®šæ¨¡å‹çš„è¶…å‚æ•° è¯å…¸æ–‡ä»¶(vocab.txt)ï¼šç”¨äºWordPiece åˆ° Word idçš„æ˜ å°„ Tensorflow checkpointï¼ˆbert_model.ckptï¼‰ï¼šåŒ…å«äº†é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ï¼ˆå®é™…åŒ…å«ä¸‰ä¸ªæ–‡ä»¶ï¼‰\r\n\r\n# **äºŒã€BERTç»“æ„**\r\n\r\nç¬¬ä¸€é˜¶æ®µï¼šPre-trainingï¼Œåˆ©ç”¨æ— æ ‡è®°çš„è¯­æ–™è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼› ç¬¬äºŒé˜¶æ®µï¼šFine-tuning, åˆ©ç”¨é¢„è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹ï¼Œå®Œæˆå…·ä½“çš„NLPä¸‹æ¸¸ä»»åŠ¡ã€‚ï¼ˆrun_classifier.pyå’Œrun_squad.pyï¼‰ extract_features.py-æå–ç‰¹å¾å‘é‡çš„\r\n\r\n# **ä¸‰ã€è¿è¡Œ**\r\n\r\n## **3.1è®­ç»ƒï¼š**\r\n\r\nrun_classifier.py è§£è¯´[[https://blog.csdn.net/weixin_41845265/article/details/107071939](https://blog.csdn.net/weixin_41845265/article/details/107071939)] æœ€å…¨è¯¦ç»†ï¼š[[https://blog.csdn.net/weixin_43320501/article/details/93894946?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control](https://blog.csdn.net/weixin_43320501/article/details/93894946?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control)]\r\n\r\n```\r\ncheckpoint è®°å½•å¯ç”¨çš„æ¨¡å‹ä¿¡æ¯\r\neval_results.txt éªŒè¯é›†çš„ç»“æœä¿¡æ¯\r\neval.tf_record è®°å½•éªŒè¯é›†çš„äºŒè¿›åˆ¶ä¿¡æ¯\r\nevents.out.tfevents.1590500393.instance-py09166k ç”¨äºtensorboardæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯\r\ngraph.pbtxt è®°å½•tensorflowçš„ç»“æ„ä¿¡æ¯\r\nlabel2id.pkl æ ‡ç­¾ä¿¡æ¯ ï¼ˆé¢å¤–åŠ çš„ï¼‰\r\nmodel.ckpt-0* è¿™é‡Œæ˜¯è®°å½•æœ€è¿‘çš„ä¸‰ä¸ªæ–‡ä»¶\r\nmodel.ckpt-2250.data æ‰€æœ‰å˜é‡çš„å€¼\r\nmodel.ckpt-2250.index å¯èƒ½æ˜¯ç”¨äºæ˜ å°„å›¾å’Œæƒé‡å…³ç³»ï¼Œ0.11ç‰ˆæœ¬åå¼•å…¥\r\nmodel.ckpt-2250.meta è®°å½•å®Œæ•´çš„è®¡ç®—å›¾ç»“æ„\r\npredict.tf_record é¢„æµ‹çš„äºŒè¿›åˆ¶æ–‡ä»¶\r\ntest_results.tsv ä½¿ç”¨é¢„æµ‹åç”Ÿæˆçš„é¢„æµ‹ç»“æœ\r\n\r\n```\r\n\r\n```\r\nexport BERT_BASE_DIR=./chinese_L-12_H-768_A-12#è¿™é‡Œæ˜¯å­˜æ”¾ä¸­æ–‡æ¨¡å‹çš„è·¯å¾„\r\nexport DATA_DIR=./data  #è¿™é‡Œæ˜¯å­˜æ”¾æ•°æ®çš„è·¯å¾„\r\n\r\npython3 run_classifier.py \\\r\n--task_name=my \\     #è¿™é‡Œæ˜¯processorçš„åå­—\r\n--do_train=true \\    #æ˜¯å¦è®­ç»ƒ\r\n--do_eval=true  \\    #æ˜¯å¦éªŒè¯\r\n--do_predict=false \\  #æ˜¯å¦é¢„æµ‹ï¼ˆå¯¹åº”testï¼‰\r\n--do_lower_case=false \\\r\n--data_dir=$DATA_DIR \\\r\n--vocab_file=$BERT_BASE_DIR/vocab.txt \\\r\n--bert_config_file=$BERT_BASE_DIR/bert_config.json \\\r\n--init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\r\n--max_seq_length=512 \\æœ€å¤§æ–‡æœ¬ç¨‹åº¦ï¼Œæœ€å¤§512\r\n--train_batch_size=4 \\\r\n--learning_rate=2e-5 \\\r\n--num_train_epochs=15 \\\r\n--output_dir=./mymodel #è¾“å‡ºç›®å½•\r\n\r\n```\r\n\r\n## **3.2æ¨¡å‹å‹ç¼©**\r\n\r\n> è¿è¡Œåä¼šåœ¨è¾“å‡ºæ–‡ä»¶å¤¹ä¸­å¤šå‡ºä¸€ä¸ª classification_model.pb æ–‡ä»¶, å°±æ˜¯å‹ç¼©åçš„æ¨¡å‹\r\n\r\n```\r\npython freeze_graph.py \\\r\n    -bert_model_dir=\"berté¢„è®­ç»ƒæ¨¡å‹åœ°å€\" \\\r\n    -model_dir=\"æ¨¡å‹è¾“å‡ºåœ°å€(å’Œä¸Šè¾¹æ¨¡å‹è®­ç»ƒè¾“å‡ºåœ°å€ä¸€æ ·å³å¯)\" \\\r\n    -max_seq_len=128 \\  # åºåˆ—é•¿åº¦, éœ€è¦ä¸è®­ç»ƒæ—¶ max_seq_length å‚ä¹¦ç›¸åŒ\r\n    -num_labels=3  # labelæ•°é‡\r\n\r\n```\r\n\r\n## **3.3æœåŠ¡éƒ¨ç½²(bert-base)**\r\n\r\n```\r\npip install bert-base==0.0.7 -i https://pypi.python.org/simple\r\nbert-base-serving-start \\\r\n    -model_dir \"è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„\" \\\r\n    -bert_model_dir \"berté¢„è®­ç»ƒæ¨¡å‹è·¯å¾„\" \\\r\n    -model_pb_dir \"classification_model.pbæ–‡ä»¶è·¯å¾„\" \\\r\n    -mode CLASS \\  # æ¨¡å¼, å’±ä»¬æ˜¯åˆ†ç±»æ‰€ä»¥ç”¨CLASS\r\n    -max_seq_len 128 \\  # åºåˆ—é•¿åº¦ä¸ä¸Šè¾¹ä¿æŒä¸€è‡´\r\n    -port 7006 \\  # ç«¯å£å·, ä¸è¦ä¸å…¶ä»–ç¨‹åºå†²çª\r\n    -port_out 7007 # ç«¯å£å·\r\n\r\n```\r\n\r\n## **3.4éƒ¨ç½²æµ‹è¯•è°ƒç”¨**\r\n\r\n```\r\nfrom bert_base.client import BertClient\r\nstr1=\"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨\"\r\nstr2 = \"å“ˆå“ˆå“ˆå“ˆ\"\r\nwith BertClient(show_server_config=False, check_version=False, check_length=False, mode=\"CLASS\", port=5575, port_out=5576) as bc:\r\n    res = bc.encode([str1, str2])\r\nprint(res)\r\n[{'pred_label': ['2', '1'], 'score': [0.9999899864196777, 0.9999299049377441]}]\r\n\r\n```\r\n\r\n## **3.5æŠ¥é”™bert_base/server/http.pyï¼š**\r\n\r\n```\r\nsudo pip install flask\r\nsudo pip install flask_compress\r\nsudo pip install flask_cors\r\nsudo pip install flask_json\r\n\r\n```\r\n\r\n##ã€€3.7é¢„æµ‹ï¼ˆæµ‹è¯•ï¼‰ï¼š\r\n\r\n> TRAINED_CLASSIFIERä¸ºåˆšåˆšè®­ç»ƒçš„è¾“å‡ºç›®å½•ï¼Œæ— éœ€åœ¨è¿›ä¸€æ­¥æŒ‡å®šæ¨¡å‹æ¨¡å‹åç§°ï¼Œå¦åˆ™åˆ†ç±»ç»“æœä¼šä¸å¯¹\r\n\r\n```\r\nexport BERT_BASE_DIR=./chinese_L-12_H-768_A-12\r\nexport DATA_DIR=./mymodel\r\nexport ./mymodel\r\npython3 run_classifier.py \\\r\n  --task_name=chi \\\r\n  --do_predict=true \\\r\n  --data_dir=$DATA_DIR \\\r\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\r\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\r\n  --init_checkpoint=$TRAINED_CLASSIFIER \\\r\n  --max_seq_length=512 \\\r\n  --output_dir=./mymodel\r\n\r\npython3 /home/zhousanfu/bert_classifier/run_classifier.py --task_name=imodis --do_predict=True --do_lower_case=False --data_dir=/data1/zhousanfu/imo_v1 --vocab_file=/data1/zhousanfu/multi_cased_L-12_H-768_A-12/vocab.txt --bert_config_file=/data1/zhousanfu/multi_cased_L-12_H-768_A-12/bert_config.json --init_checkpoint=/data1/zhousanfu/multi_cased_L-12_H-768_A-12/bert_model.ckpt --max_seq_length=128 --output_dir=/data1/zhousanfu/imo_v7\r\n\r\n```\r\n\r\n## **3.8 å¦ä¸€ç§æœåŠ¡TF-serving éƒ¨ç½²æ¨¡å‹[[https://blog.csdn.net/qq_42693848/article/details/107235688](https://blog.csdn.net/qq_42693848/article/details/107235688)]**\r\n\r\n- ([https://blog.csdn.net/JerryZhang__/article/details/85107506?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.control](https://blog.csdn.net/JerryZhang__/article/details/85107506?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.control))\r\n\r\n> ä½¿ç”¨dockerä¸‹è½½tfservingé•œåƒ\r\n\r\n```\r\ndocker pull tensorflow/serving:2.1.0  # è¿™é‡Œä¸‹è½½çš„æ˜¯tf2.1.0çš„ç‰ˆæœ¬ï¼Œæ”¯æŒtensorflow1.15ä»¥ä¸Šè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ã€‚\r\ndocker run\r\n    -p 8501:8501\r\n    -p 8500:8500\r\n    --mount type=bind,source=/my/model/path/m,target=/models/m\r\n    -e MODEL_NAME=m\r\n    -t tensorflow/serving:2.1.0\r\nä¸Šé¢çš„å‘½ä»¤ä¸­ï¼š\r\n(1)-p 8501:8501æ˜¯ç«¯å£æ˜ å°„ï¼Œæ˜¯å°†å®¹å™¨çš„8501ç«¯å£æ˜ å°„åˆ°å®¿ä¸»æœºçš„8501ç«¯å£ï¼Œåé¢é¢„æµ‹çš„æ—¶å€™ä½¿ç”¨è¯¥ç«¯å£ï¼›\r\n(2)-e MODEL_NAME=testnet è®¾ç½®æ¨¡å‹åç§°ï¼›\r\n(3)--mount type=bind,source=/tmp/testnet,target=/models/testnet æ˜¯å°†å®¿ä¸»æœºçš„è·¯å¾„/tmp/testnetæŒ‚è½½åˆ°å®¹å™¨çš„/models/testnetä¸‹ã€‚/tmp/testnetæ˜¯å­˜æ”¾çš„æ˜¯ä¸Šè¿°å‡†å¤‡å·¥ä½œä¸­ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶ï¼Œâ€˜testnetâ€™æ˜¯æ¨¡å‹åç§°ï¼ŒåŒ…å«ä¸€ä¸ª.pbæ–‡ä»¶å’Œä¸€ä¸ªvariablesæ–‡ä»¶å¤¹ï¼Œåœ¨/tmp/testnetä¸‹æ–°å»ºä¸€ä¸ªä»¥æ•°å­—å‘½åçš„æ–‡ä»¶å¤¹ï¼Œå¦‚100001ï¼Œå¹¶å°†æ¨¡å‹æ–‡ä»¶æ”¾åˆ°è¯¥æ–‡ä»¶å¤¹ä¸­ã€‚å®¹å™¨å†…éƒ¨ä¼šæ ¹æ®ç»‘å®šçš„è·¯å¾„è¯»å–æ¨¡å‹æ–‡ä»¶ï¼›\r\n(4)-t tensorflow/serving æ ¹æ®åç§°â€œtensorflow/servingâ€è¿è¡Œå®¹å™¨ï¼›\r\n\r\n$ docker run -p 8501:8501 --mount type=bind,source=/tmp/testnet,target=/models/testnet  -e MODEL_NAME=bert_NLP_\r\n```",
      "data": {
        "title": "BERT æ¨¡å‹æ–‡æœ¬åˆ†ç±»è®­ç»ƒä¸éƒ¨ç½²",
        "date": "2021-01-26 16:32:39",
        "tags": [
          "NLP"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "nlp-bert-mo-xing-xun-lian-zhi-bu-shu"
    },
    {
      "content": "# **ç®€å•ä½¿ç”¨**\r\n\r\n1. å®‰è£…è½¯ä»¶ï¼šbrew install è½¯ä»¶åï¼Œä¾‹ï¼šbrew install wget\r\n2. æœç´¢è½¯ä»¶ï¼šbrew search è½¯ä»¶åï¼Œä¾‹ï¼šbrew search wget\r\n3. å¸è½½è½¯ä»¶ï¼šbrew uninstall è½¯ä»¶åï¼Œä¾‹ï¼šbrew uninstall wget\r\n4. æ›´æ–°æ‰€æœ‰è½¯ä»¶ï¼šbrew update\r\n5. æ›´æ–°å…·ä½“è½¯ä»¶ï¼šbrew upgrade è½¯ä»¶å ï¼Œä¾‹ï¼šbrew upgrade git\r\n6. æ˜¾ç¤ºå·²å®‰è£…è½¯ä»¶ï¼šbrew list\r\n7. æŸ¥çœ‹è½¯ä»¶ä¿¡æ¯ï¼šbrew infoï¼home è½¯ä»¶å ï¼Œä¾‹ï¼šbrew info git ï¼ brew home git\r\n8. PSï¼šbrew homeæŒ‡ä»¤æ˜¯ç”¨æµè§ˆå™¨æ‰“å¼€å®˜æ–¹ç½‘é¡µæŸ¥çœ‹è½¯ä»¶ä¿¡æ¯\r\n9. æŸ¥çœ‹å“ªäº›å·²å®‰è£…çš„ç¨‹åºéœ€è¦æ›´æ–°ï¼š brew outdated\r\n10. æ˜¾ç¤ºåŒ…ä¾èµ–ï¼šbrew reps\r\n11. æ˜¾ç¤ºå¸®åŠ©ï¼šbrew help\r\n\r\n# äºŒã€ARM X86å®‰è£…ä¸é…ç½®\r\n\r\n### Armç‰ˆå®‰è£…ï¼š\r\n\r\n```bash\r\ncd /opt # åˆ‡æ¢åˆ° /opt ç›®å½•\r\nmkdir homebrew # åˆ›å»º homebrew ç›®å½•\r\nsudo chown -R $(whoami) /opt/homebrew # ä¿®æ”¹ç›®å½•æ‰€å±ç”¨æˆ·\r\ncurl -L [https://github.com/Homebrew/brew/tarball/master](https://github.com/Homebrew/brew/tarball/master) | tar xz --strip 1 -C homebrew\r\n```\r\n\r\n### å¸è½½å‘½ä»¤\r\n\r\n```bash\r\n/usr/bin/ruby -e \"(surl -fsSL https://...)\"\r\n```\r\n\r\n### intelç‰ˆçš„å®‰è£…ï¼š\r\n\r\n```bash\r\n```shell\r\n\r\narch -x86_64 /bin/bash -c \"$(curl -fsSL [https://raw.githubusercontent.com/Homebrew/install/master/install.sh](https://raw.githubusercontent.com/Homebrew/install/master/install.sh))\"\r\n\r\n```\r\n```\r\n\r\n### ç¯å¢ƒå˜é‡è®¾ç½®\r\n\r\n```bash\r\n#vim ~/.zshrc æˆ–è€… vim ~/.bash_profile\r\n\r\n# x86\r\n$ export PATH=\"/usr/local/bin:$PATH\"\r\n$ alias abrew='arch -x86_64 /usr/local/bin/brew'\r\n\r\n# arm\r\n$ export PATH=\"/opt/homebrew/bin:$PATH\"\r\n$ alias brew='/opt/homebrew/bin/brew'\r\n```\r\n\r\n### æ¢æºä¸‹è½½æº\r\n\r\n```bash\r\n# Arm ç‰ˆ\r\n\r\ncd \"$(brew --repo)\"\r\ngit remote set-url origin [git://mirrors](git://mirrors).[ustc.edu.cn/brew.git](http://ustc.edu.cn/brew.git)\r\ncd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\"\r\ngit remote set-url origin [git://mirrors](git://mirrors).[ustc.edu.cn/homebrew-core.git](http://ustc.edu.cn/homebrew-core.git)\r\n\r\n# intelç‰ˆ\r\ncd \"$(abrew --repo)\"\r\ngit remote set-url origin [git://mirrors](git://mirrors).[ustc.edu.cn/brew.git](http://ustc.edu.cn/brew.git)\r\ncd \"$(abrew --repo)/Library/Taps/homebrew/homebrew-core\"\r\ngit remote set-url origin [git://mirrors](git://mirrors).[ustc.edu.cn/homebrew-core.git](http://ustc.edu.cn/homebrew-core.git)\r\n\r\nbashç”¨æˆ·ï¼š\r\necho 'export HOMEBREW_BOTTLE_DOMAIN=[https://mirrors](https://mirrors/).[ustc.edu.cn/homebrew-bottles](http://ustc.edu.cn/homebrew-bottles)' >> ~/.bash_profile\r\nsource ~/.bash_profile\r\nzshç”¨æˆ·ï¼š\r\necho 'export HOMEBREW_BOTTLE_DOMAIN=[https://mirrors](https://mirrors/).[ustc.edu.cn/homebrew-bottles](http://ustc.edu.cn/homebrew-bottles)' >> ~/.zshrc\r\nsource ~/.zshrc\r\n```",
      "data": {
        "title": "Mac Homebrew å®‰è£…ä¸ä½¿ç”¨",
        "date": "2021-01-15 12:09:09",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "mac-homebrew-an-zhuang-yu-shi-yong"
    },
    {
      "content": "- **æ–‡æœ¬è¿‡æ»¤**\r\n\r\n    ```python\r\n    def review_to_words(data):\r\n        \r\n        # æ­£åˆ™å»é™¤è¡¨æƒ…\r\n        emoji_pattern = re.compile(u'[\\U00010000-\\U0010ffff]')\r\n        data = emoji_pattern.sub(u'', data)\r\n        \r\n        # æ­£åˆ™å»é™¤æ ‡ç‚¹\r\n        fuhao_pattern = re.compile(u'\\.*')\r\n        data = fuhao_pattern.sub(u'', data)\r\n        \r\n        # æ­£åˆ™å»é™¤æ•°å­—\r\n        digit_pattern = re.compile(u'\\d+')\r\n        data = digit_pattern.sub(u'', data)\r\n        \r\n        # ç©ºæ ¼æ‹†åˆ†è¯è¯­\r\n        words = data.lower().split()\r\n        \r\n        # å»æ‰rmSignal\r\n        meaningful_words = [w for w in words if not w in rmSignal]\r\n        \r\n        # å°†ç­›åˆ†å¥½çš„è¯åˆæˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶ç”¨ç©ºæ ¼åˆ†å¼€\r\n        words = \" \".join(meaningful_words)\r\n        return words\r\n    ```",
      "data": {
        "title": "æ•°æ®æ¸…æ´—",
        "date": "2021-01-03 12:17:54",
        "tags": [
          "æ•°æ®æŒ–æ˜"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "chang-yong-shu-ju-qing-xi"
    },
    {
      "content": "\r\n#****gitæäº¤æ–‡ä»¶****\r\n\r\n- æ·»åŠ æ–‡ä»¶ï¼š`git add æ–‡ä»¶ågit add -A`Â ä¸€é”®add\r\n- æäº¤æ–‡ä»¶:`git commit -m \"æäº¤æ–‡ä»¶æ—¶çš„è¯´æ˜\"`\r\n- æ¨é€åˆ°è¿œç¨‹ä»“åº“:æ™®é€šæ¨é€:Â `git push`å¼ºè¡Œæ¨é€:Â `git push -u origin master -f`\r\n\r\n**ä¸€ã€æœ¬åœ°æ“ä½œï¼š**\r\n\r\n**1.å…¶å®ƒ**\r\n\r\ngit initï¼šåˆå§‹åŒ–æœ¬åœ°åº“\r\n\r\ngit statusï¼šæŸ¥çœ‹å·¥ä½œåŒºã€æš‚å­˜åŒºçš„çŠ¶æ€\r\n\r\ngit add <file name>ï¼šå°†å·¥ä½œåŒºçš„â€œæ–°å»º/ä¿®æ”¹â€æ·»åŠ åˆ°æš‚å­˜åŒº\r\n\r\ngit rm --cached <file name>ï¼šç§»é™¤æš‚å­˜åŒºçš„ä¿®æ”¹\r\n\r\ngit commit <file name>ï¼šå°†æš‚å­˜åŒºçš„å†…å®¹æäº¤åˆ°æœ¬åœ°åº“\r\n\r\ntipï¼šéœ€è¦å†ç¼–è¾‘æäº¤æ—¥å¿—ï¼Œæ¯”è¾ƒéº»çƒ¦ï¼Œå»ºè®®ç”¨ä¸‹é¢å¸¦å‚æ•°çš„æäº¤æ–¹æ³•\r\n\r\ngit commit -m \"æäº¤æ—¥å¿—\" <file name>ï¼šæ–‡ä»¶ä»æš‚å­˜åŒºåˆ°æœ¬åœ°åº“\r\n\r\n**2.æ—¥å¿—**\r\n\r\ngit logï¼šæŸ¥çœ‹å†å²æäº¤\r\n\r\ntipï¼šç©ºæ ¼å‘ä¸‹ç¿»é¡µï¼Œbå‘ä¸Šç¿»é¡µï¼Œqé€€å‡º\r\n\r\ngit log --pretty=onelineï¼šä»¥æ¼‚äº®çš„ä¸€è¡Œæ˜¾ç¤ºï¼ŒåŒ…å«å…¨éƒ¨å“ˆå¸Œç´¢å¼•å€¼\r\n\r\ngit log --onelineï¼šä»¥ç®€æ´çš„ä¸€è¡Œæ˜¾ç¤ºï¼ŒåŒ…å«ç®€æ´å“ˆå¸Œç´¢å¼•å€¼\r\n\r\ngit reflogï¼šä»¥ç®€æ´çš„ä¸€è¡Œæ˜¾ç¤ºï¼ŒåŒ…å«ç®€æ´å“ˆå¸Œç´¢å¼•å€¼ï¼ŒåŒæ—¶æ˜¾ç¤ºç§»åŠ¨åˆ°æŸä¸ªå†å²ç‰ˆæœ¬æ‰€éœ€çš„æ­¥æ•°\r\n\r\n**3.ç‰ˆæœ¬æ§åˆ¶**\r\n\r\ngit reset --hard ç®€æ´/å®Œæ•´å“ˆå¸Œç´¢å¼•å€¼ï¼šå›åˆ°æŒ‡å®šå“ˆå¸Œå€¼æ‰€å¯¹åº”çš„ç‰ˆæœ¬\r\n\r\ngit reset --hard HEADï¼šå¼ºåˆ¶å·¥ä½œåŒºã€æš‚å­˜åŒºã€æœ¬åœ°åº“ä¸ºå½“å‰HEADæŒ‡é’ˆæ‰€åœ¨çš„ç‰ˆæœ¬\r\n\r\ngit reset --hard HEAD^ï¼šåé€€ä¸€ä¸ªç‰ˆæœ¬\r\n\r\ntipï¼šä¸€ä¸ª^è¡¨ç¤ºå›é€€ä¸€ä¸ªç‰ˆæœ¬\r\n\r\ngit reset --hard HEAD~1ï¼šåé€€ä¸€ä¸ªç‰ˆæœ¬\r\n\r\ntipï¼šæ³¢æµªçº¿~åé¢çš„æ•°å­—è¡¨ç¤ºåé€€å‡ ä¸ªç‰ˆæœ¬\r\n\r\n**4.æ¯”è¾ƒå·®å¼‚**\r\n\r\ngit diffï¼šæ¯”è¾ƒå·¥ä½œåŒºå’Œæš‚å­˜åŒºçš„**æ‰€æœ‰æ–‡ä»¶**å·®å¼‚\r\n\r\ngit diff <file name>ï¼šæ¯”è¾ƒå·¥ä½œåŒºå’Œæš‚å­˜åŒºçš„**æŒ‡å®šæ–‡ä»¶**çš„å·®å¼‚\r\n\r\ngit diff HEAD|HEAD^|HEAD~|å“ˆå¸Œç´¢å¼•å€¼Â <file name>ï¼šæ¯”è¾ƒå·¥ä½œåŒºè·Ÿæœ¬åœ°åº“çš„æŸä¸ªç‰ˆæœ¬çš„**æŒ‡å®šæ–‡ä»¶**çš„å·®å¼‚\r\n\r\n**5.åˆ†æ”¯æ“ä½œ**\r\n\r\ngit branch -vï¼šæŸ¥çœ‹æ‰€æœ‰åˆ†æ”¯\r\n\r\ngit branch -d <åˆ†æ”¯å>ï¼šåˆ é™¤æœ¬åœ°åˆ†æ”¯\r\n\r\ngit branch <åˆ†æ”¯å>ï¼šæ–°å»ºåˆ†æ”¯\r\n\r\ngit checkout <åˆ†æ”¯å>ï¼šåˆ‡æ¢åˆ†æ”¯\r\n\r\ngit merge <è¢«åˆå¹¶åˆ†æ”¯å>ï¼šåˆå¹¶åˆ†æ”¯\r\n\r\ntipï¼šå¦‚masteråˆ†æ”¯åˆå¹¶ hot_fixåˆ†æ”¯ï¼Œé‚£ä¹ˆå½“å‰å¿…é¡»å¤„äºmasteråˆ†æ”¯ä¸Šï¼Œç„¶åæ‰§è¡Œ git merge hot_fix å‘½ä»¤\r\n\r\ntip2ï¼šåˆå¹¶å‡ºç°å†²çª\r\n\r\nâ‘ åˆ é™¤gitè‡ªåŠ¨æ ‡è®°ç¬¦å·ï¼Œå¦‚<<<<<<< HEADã€>>>>>>>ç­‰\r\n\r\nâ‘¡ä¿®æ”¹åˆ°æ»¡æ„åï¼Œä¿å­˜é€€å‡º\r\n\r\nâ‘¢git add <file name>\r\n\r\nâ‘£git commit -m \"æ—¥å¿—ä¿¡æ¯\"ï¼Œæ­¤æ—¶åé¢ä¸è¦å¸¦æ–‡ä»¶å\r\n\r\n**äºŒã€æœ¬åœ°åº“è·Ÿè¿œç¨‹åº“äº¤äº’ï¼š**\r\n\r\ngit clone <è¿œç¨‹åº“åœ°å€>ï¼šå…‹éš†è¿œç¨‹åº“\r\n\r\nåŠŸèƒ½ï¼šâ‘ å®Œæ•´çš„å…‹éš†è¿œç¨‹åº“ä¸ºæœ¬åœ°åº“ï¼Œâ‘¡ä¸ºæœ¬åœ°åº“æ–°å»ºoriginåˆ«åï¼Œâ‘¢åˆå§‹åŒ–æœ¬åœ°åº“\r\n\r\ngit remote -vï¼šæŸ¥çœ‹è¿œç¨‹åº“åœ°å€åˆ«å\r\n\r\ngit remote add <åˆ«å> <è¿œç¨‹åº“åœ°å€>ï¼šæ–°å»ºè¿œç¨‹åº“åœ°å€åˆ«å\r\n\r\ngit remote rm <åˆ«å>ï¼šåˆ é™¤æœ¬åœ°ä¸­è¿œç¨‹åº“åˆ«å\r\n\r\ngit push <åˆ«å> <åˆ†æ”¯å>ï¼šæœ¬åœ°åº“æŸä¸ªåˆ†æ”¯æ¨é€åˆ°è¿œç¨‹åº“ï¼Œåˆ†æ”¯å¿…é¡»æŒ‡å®š\r\n\r\ngit pull <åˆ«å> <åˆ†æ”¯å>ï¼šæŠŠè¿œç¨‹åº“çš„ä¿®æ”¹æ‹‰å–åˆ°æœ¬åœ°\r\n\r\ntipï¼šè¯¥å‘½ä»¤åŒ…æ‹¬git fetchï¼Œgit merge\r\n\r\ngit fetch <è¿œç¨‹åº“åˆ«å> <è¿œç¨‹åº“åˆ†æ”¯å>ï¼šæŠ“å–è¿œç¨‹åº“çš„æŒ‡å®šåˆ†æ”¯åˆ°æœ¬åœ°ï¼Œä½†æ²¡æœ‰åˆå¹¶\r\n\r\ngit merge <è¿œç¨‹åº“åˆ«å/è¿œç¨‹åº“åˆ†æ”¯å>ï¼šå°†æŠ“å–ä¸‹æ¥çš„è¿œç¨‹çš„åˆ†æ”¯ï¼Œè·Ÿå½“å‰æ‰€åœ¨åˆ†æ”¯è¿›è¡Œåˆå¹¶\r\n\r\ngit forkï¼šå¤åˆ¶è¿œç¨‹åº“\r\n\r\ntipï¼šä¸€èˆ¬æ˜¯å¤–é¢å›¢é˜Ÿçš„å¼€å‘äººå‘˜forkæœ¬å›¢é˜Ÿé¡¹ç›®ï¼Œç„¶åè¿›è¡Œå¼€å‘ï¼Œä¹‹åå¤–é¢å›¢é˜Ÿå‘èµ·pull requestï¼Œç„¶åæœ¬å›¢é˜Ÿè¿›è¡Œå®¡æ ¸ï¼Œå¦‚æ— é—®é¢˜æœ¬å›¢é˜Ÿè¿›è¡Œmergeï¼ˆåˆå¹¶ï¼‰åˆ°å›¢é˜Ÿè‡ªå·±çš„è¿œç¨‹åº“ï¼Œæ•´ä¸ªæµç¨‹å°±æ˜¯æœ¬å›¢é˜Ÿè·Ÿå¤–é¢å›¢é˜Ÿçš„ååŒå¼€å‘æµç¨‹ï¼ŒLinuxçš„å›¢é˜Ÿå¼€å‘æˆå‘˜å³ä¸ºè¿™ç§å·¥ä½œæ–¹å¼ã€‚\r\n\r\n### ä¸‰ã€åˆ›å»ºgit\r\n\r\n```bash\r\ngit remote add origin https://github.com/zhousanfu/digital_monitoring_liunx.git\r\ngit push origin master\r\n```",
      "data": {
        "title": "Git ä½¿ç”¨ä¹‹å‘½ä»¤è¡Œç¯‡",
        "date": "2020-02-26 17:06:14",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "git-shi-yong-ming-zhi-ling-xing-pian"
    },
    {
      "content": "Jupyter Notebook æ˜¯æ•°æ®ç§‘å­¦ / æœºå™¨å­¦ä¹ ç¤¾åŒºå†…ä¸€æ¬¾éå¸¸æµè¡Œçš„å·¥å…·\r\n\r\næœ€è¿‘å…¥æ‰‹äº†ä¸€å°æ–°çš„æœåŠ¡å™¨ï¼Œäºæ˜¯æˆ‘æƒ³åˆ°å°† Jupyter æ­å»ºåˆ°æœåŠ¡å™¨ä¸Šï¼Œåœ¨ä»»ä½•åªè¦æœ‰æµè§ˆå™¨çš„åœ°æ–¹éƒ½èƒ½è¿›è¡Œ Python ç¼–ç¨‹\r\n\r\næˆ‘çš„æœåŠ¡å™¨æ˜¯ ubuntu çš„ï¼Œä¸åŒç³»ç»Ÿæ ¹æ®è‡ªå·±ç³»ç»Ÿçš„å‘½ä»¤è¿›è¡Œæ“ä½œ\r\n\r\n### **åˆ›å»ºç”¨æˆ·ã€åˆ‡æ¢ç”¨æˆ·å±•å¼€ç›®å½•**\r\n\r\né¦–å…ˆåœ¨ root ç”¨æˆ·ä¸‹æ‰“å¼€é˜²ç«å¢™ 8888 ç«¯å£ï¼Œè¿™æ˜¯æä¾› Jupyter æœåŠ¡çš„ç«¯å£\r\n\r\n```\r\nsudo ufw allow 8888\r\n\r\n```\r\n\r\nç„¶ååˆ›å»ºä¸€ä¸ªç”¨æˆ·åä¸ºÂ `mathor`Â çš„ç”¨æˆ·\r\n\r\n```\r\nsudo adduser mathor\r\n\r\n```\r\n\r\nè¾“å…¥å¯†ç å¹¶ç¡®è®¤å¯†ç \r\n\r\nç„¶åä¸€è·¯ Enterï¼Œé»˜è®¤å°±è¡Œï¼Œæœ€åè¾“å…¥Â `y`Â ç¡®è®¤ä¸€ä¸‹\r\n\r\nç„¶ååˆ‡æ¢åˆ°æ–°ç”¨æˆ·ï¼Œå¹¶è¿›å…¥å½“å‰ç”¨æˆ·çš„ä¸»ç›®å½•\r\n\r\n```\r\nsu mathor\r\n\r\ncd ~\r\n\r\n```\r\n\r\n### **ä¸‹è½½å¹¶å®‰è£… Anacondaå±•å¼€ç›®å½•**\r\n\r\nAnaconda çš„ Linux ä¸‹è½½ç½‘å€æ˜¯Â **[https://www.anaconda.com/download/#linux](https://www.anaconda.com/download/#linux)**\r\n\r\næˆªæ­¢åˆ°ä»Šå¤©çš„æœ€æ–°ç‰ˆæœ¬æ˜¯ 2018.12ï¼Œæ‰€ä»¥é€šè¿‡å‘½ä»¤ä¸‹è½½\r\n\r\n```\r\nwget https://repo.continuum.io/archive/Anaconda3-2018.12-Linux-x86_64.sh\r\n\r\n```\r\n\r\nä¸‹è½½å®Œæˆåè¿è¡Œ\r\n\r\n```\r\nbash Anaconda3-2018.12-Linux-x86_64.sh\r\n\r\n```\r\n\r\nä¹‹åä¼šæœ‰ä¸€ä¸ªåè®®ï¼Œè¾“å…¥Â `yes`ï¼Œç„¶åä¼šæœ‰å®‰è£…è·¯å¾„é€‰æ‹©ï¼ŒæŒ‰ä¸‹ Enter å°±æ˜¯é»˜è®¤è·¯å¾„ï¼Œä¹‹åä¼šé—®æ˜¯å¦è¦åŠ å…¥åˆ°ç¯å¢ƒå˜é‡ï¼Œè¾“å…¥Â `yes`ï¼Œä¹‹åé—®è¦ä¸è¦å®‰è£… vs codeï¼Œè¾“å…¥Â `no`ã€‚æœ€åå®‰è£…å®Œæˆï¼Œè¾“å…¥\r\n\r\n```\r\njupyter\r\n\r\n```\r\n\r\næŒ‰ä¸¤ä¸‹ tab é”®æç¤ºå¾ˆå¤šä¸œè¥¿ï¼Œå°±è¯æ˜é€šè¿‡ Anaconda å®‰è£… Jupyter æˆåŠŸäº†ï¼Œå¦‚æœæ²¡æœ‰ååº”ï¼ŒåŒæ—¶å‘ç°è¾“å…¥Â `conda`Â æ²¡æœ‰å‘½ä»¤ï¼Œé‚£ä¹ˆæ‰§è¡Œä¸‹é¢ä¸¤æ­¥å°±å¯ä»¥äº†\r\n\r\n```\r\necho 'export PATH=\"~/anaconda3/bin:$PATH\"'>>~/.bashrc\r\n\r\nsource ~/.bashrc\r\n\r\n```\r\n\r\n### **é…ç½® Jupyterå±•å¼€ç›®å½•**\r\n\r\nè¿è¡Œå‘½ä»¤\r\n\r\n```\r\njupyter-notebook --generate-config\r\n\r\n```\r\n\r\nè¿™æ—¶çœ‹åˆ°ä¸€ä¸ªåé¦ˆ\r\n\r\n```\r\nWritingdefault config to:/home/mathor/.jupyter/jupyter_notebook_config.py\r\n\r\n```\r\n\r\nè¿™å°±æ˜¯é…ç½®çš„ç›®å½•ã€‚ç„¶åè¿è¡Œå‘½ä»¤\r\n\r\n```\r\njupyter-notebook password\r\n\r\n```\r\n\r\nè¾“å…¥å¯†ç å¹¶ç¡®è®¤ï¼Œè¿™å°±æ˜¯ä»¥åç™»é™†çš„å¯†ç \r\n\r\nè¾“å…¥å‘½ä»¤\r\n\r\n```\r\nvi .jupyter/jupyter_notebook_config.json\r\n\r\n```\r\n\r\nå¯ä»¥çœ‹åˆ°æœ‰ä¸€ä¸ªå­—ç¬¦ä¸²Â `sha1:xxxxxxx`ï¼Œå¤åˆ¶ä¸‹æ¥ï¼Œä¸€ä¼šè¦ç”¨åˆ°ã€‚ç„¶åè¿è¡Œå‘½ä»¤\r\n\r\n```\r\nmkdir jupyterdata\r\n\r\n```\r\n\r\nåˆ›é€ ä¸€ä¸ªæ–‡ä»¶å¤¹å­˜æ”¾ jupyter çš„ä»£ç ã€‚æœ€åé…ç½®ç«¯å£ä¸ä»£ç å­˜æ”¾è·¯å¾„\r\n\r\n```\r\nvi .jupyter/jupyter_notebook_config.py\r\n\r\n```\r\n\r\néšä¾¿åœ¨ç©ºç™½å¤„å†™ä¸Šä¸‹é¢çš„å…³é”®é…ç½®å³å¯ï¼š\r\n\r\n```\r\n# è®¾ç½®é»˜è®¤ç›®å½•\r\n\r\nc.NotebookApp.notebook_dir = u'/home/mathor/jupyterdata'\r\n\r\n# å…è®¸é€šè¿‡ä»»æ„ç»‘å®šæœåŠ¡å™¨çš„ipè®¿é—®\r\n\r\nc.NotebookApp.ip = '*'\r\n\r\n# ç”¨äºè®¿é—®çš„ç«¯å£\r\n\r\nc.NotebookApp.port = 8888\r\n\r\n# ä¸è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨\r\n\r\nc.NotebookApp.open_browser = False\r\n\r\n# è®¾ç½®ç™»å½•å¯†ç \r\n\r\nc.NotebookApp.password = u'sha1:xxxxxxxxxxxxxxxx'\r\n\r\n```\r\n\r\nä¿å­˜å¹¶é€€å‡ºï¼ˆæŒ‰ä¸‹ ESCï¼Œè¾“å…¥`:wq`Â å›è½¦ï¼‰\r\n\r\nç„¶åè¿è¡Œ\r\n\r\n```\r\njupyter-notebook\r\n\r\n```\r\n\r\nå¦‚æœå‡ºç°ä¸‹é¢çš„æŠ¥é”™\r\n\r\n```\r\nPermissionError: [Errno 13] Permission denied: '/run/user/xxxx/jupyter'\r\n\r\n```\r\n\r\nåˆ™è¾“å…¥\r\n\r\n```\r\nexport XDG_RUNTIME_DIR=\"/home/mathor/anaconda3\"\r\n\r\nsource .bashrc\r\n\r\njupyter-notebook\r\n\r\n```\r\n\r\nç„¶åå°± OK äº†\r\n\r\n### **æœ¬åœ°æµ‹è¯•å±•å¼€ç›®å½•**\r\n\r\néšä¾¿åœ¨ä¸€ä¸ªå®¢æˆ·æœºæµè§ˆå™¨é‡Œè¾“å…¥Â `http://ip:8888`Â å°±å¯ä»¥è¿›å…¥ Jupyter çš„ç™»é™†ç•Œé¢äº†",
      "data": {
        "title": "æ­å»ºäº‘ç«¯ Jupyter",
        "date": "2020-02-09 12:28:42",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "da-jian-yun-duan-jupyter"
    },
    {
      "content": "### **Byte Pair Encoding**\r\n\r\nåœ¨ NLP æ¨¡å‹ä¸­ï¼Œè¾“å…¥é€šå¸¸æ˜¯ä¸€ä¸ªå¥å­ï¼Œä¾‹å¦‚Â `\"I went to New York last week.\"`ï¼Œä¸€å¥è¯ä¸­åŒ…å«å¾ˆå¤šå•è¯ï¼ˆtokenï¼‰ã€‚ä¼ ç»Ÿçš„åšæ³•æ˜¯å°†è¿™äº›å•è¯ä»¥ç©ºæ ¼è¿›è¡Œåˆ†éš”ï¼Œä¾‹å¦‚Â `['i', 'went', 'to', 'New', 'York', 'last', 'week']`ã€‚ç„¶è€Œè¿™ç§åšæ³•å­˜åœ¨å¾ˆå¤šé—®é¢˜ï¼Œä¾‹å¦‚æ¨¡å‹æ— æ³•é€šè¿‡Â `old, older, oldest`Â ä¹‹é—´çš„å…³ç³»å­¦åˆ°Â `smart, smarter, smartest`Â ä¹‹é—´çš„å…³ç³»ã€‚å¦‚æœæˆ‘ä»¬èƒ½ä½¿ç”¨å°†ä¸€ä¸ª token åˆ†æˆå¤šä¸ª subtokensï¼Œä¸Šé¢çš„é—®é¢˜å°±èƒ½å¾ˆå¥½çš„è§£å†³ã€‚æœ¬æ–‡å°†è¯¦è¿°ç›®å‰æ¯”è¾ƒå¸¸ç”¨çš„ subtokens ç®—æ³• â€”â€”BPEï¼ˆByte-Pair Encodingï¼‰\r\n\r\nç°åœ¨æ€§èƒ½æ¯”è¾ƒå¥½ä¸€äº›çš„ NLP æ¨¡å‹ï¼Œä¾‹å¦‚ GPTã€BERTã€RoBERTa ç­‰ï¼Œåœ¨æ•°æ®é¢„å¤„ç†çš„æ—¶å€™éƒ½ä¼šæœ‰ WordPiece çš„è¿‡ç¨‹ï¼Œå…¶ä¸»è¦çš„å®ç°æ–¹å¼å°±æ˜¯ BPEï¼ˆByte-Pair Encodingï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œä¾‹å¦‚Â `['loved', 'loving', 'loves']`Â è¿™ä¸‰ä¸ªå•è¯ã€‚å…¶å®æœ¬èº«çš„è¯­ä¹‰éƒ½æ˜¯ \"çˆ±\" çš„æ„æ€ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬ä»¥è¯ä¸ºå•ä½ï¼Œé‚£å®ƒä»¬å°±ç®—ä¸ä¸€æ ·çš„è¯ï¼Œåœ¨è‹±è¯­ä¸­ä¸åŒåç¼€çš„è¯éå¸¸çš„å¤šï¼Œå°±ä¼šä½¿å¾—è¯è¡¨å˜çš„å¾ˆå¤§ï¼Œè®­ç»ƒé€Ÿåº¦å˜æ…¢ï¼Œè®­ç»ƒçš„æ•ˆæœä¹Ÿä¸æ˜¯å¤ªå¥½ã€‚BPE ç®—æ³•é€šè¿‡è®­ç»ƒï¼Œèƒ½å¤ŸæŠŠä¸Šé¢çš„ 3 ä¸ªå•è¯æ‹†åˆ†æˆÂ `[\"lov\",\"ed\",\"ing\",\"es\"]`Â å‡ éƒ¨åˆ†ï¼Œè¿™æ ·å¯ä»¥æŠŠè¯çš„æœ¬èº«çš„æ„æ€å’Œæ—¶æ€åˆ†å¼€ï¼Œæœ‰æ•ˆçš„å‡å°‘äº†è¯è¡¨çš„æ•°é‡ã€‚ç®—æ³•æµç¨‹å¦‚ä¸‹ï¼š\r\n\r\n1. è®¾å®šæœ€å¤§ subwords ä¸ªæ•°\r\n\r\n    V\r\n\r\n2. å°†æ‰€æœ‰å•è¯æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦ï¼Œå¹¶åœ¨æœ€åæ·»åŠ ä¸€ä¸ªåœæ­¢ç¬¦Â `</w>`ï¼ŒåŒæ—¶æ ‡è®°å‡ºè¯¥å•è¯å‡ºç°çš„æ¬¡æ•°ã€‚ä¾‹å¦‚ï¼Œ`\"low\"`Â è¿™ä¸ªå•è¯å‡ºç°äº† 5 æ¬¡ï¼Œé‚£ä¹ˆå®ƒå°†ä¼šè¢«å¤„ç†ä¸ºÂ `{'l o w </w>': 5}`\r\n3. ç»Ÿè®¡æ¯ä¸€ä¸ªè¿ç»­**å­—èŠ‚å¯¹**çš„å‡ºç°é¢‘ç‡ï¼Œé€‰æ‹©æœ€é«˜é¢‘è€…åˆå¹¶æˆæ–°çš„ subword\r\n4. é‡å¤ç¬¬ 3 æ­¥ç›´åˆ°è¾¾åˆ°ç¬¬ 1 æ­¥è®¾å®šçš„ subwords è¯è¡¨å¤§å°æˆ–ä¸‹ä¸€ä¸ªæœ€é«˜é¢‘çš„**å­—èŠ‚å¯¹**å‡ºç°é¢‘ç‡ä¸º 1\r\n\r\nä¾‹å¦‚\r\n\r\n```\r\n{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\r\n\r\n```\r\n\r\nå‡ºç°æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹æ˜¯**`e`**å’Œ**`s`**ï¼Œå…±å‡ºç°äº† 6+3=9 æ¬¡ï¼Œå› æ­¤å°†å®ƒä»¬åˆå¹¶\r\n\r\n```\r\n{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\r\n\r\n```\r\n\r\nå‡ºç°æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹æ˜¯**`es`**å’Œ**`t`**ï¼Œå…±å‡ºç°äº† 6+3=9 æ¬¡ï¼Œå› æ­¤å°†å®ƒä»¬åˆå¹¶\r\n\r\n```\r\n{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\r\n\r\n```\r\n\r\nå‡ºç°æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹æ˜¯**`est`**å’Œ**`</w>`**ï¼Œå…±å‡ºç°äº† 6+3=9 æ¬¡ï¼Œå› æ­¤å°†å®ƒä»¬åˆå¹¶\r\n\r\n```\r\n{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\r\n\r\n```\r\n\r\nå‡ºç°æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹æ˜¯**`l`**å’Œ**`o`**ï¼Œå…±å‡ºç°äº† 5+2=7 æ¬¡ï¼Œå› æ­¤å°†å®ƒä»¬åˆå¹¶\r\n\r\n```\r\n{'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\r\n\r\n```\r\n\r\nå‡ºç°æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹æ˜¯**`lo`**å’Œ**`w`**ï¼Œå…±å‡ºç°äº† 5+2=7 æ¬¡ï¼Œå› æ­¤å°†å®ƒä»¬åˆå¹¶\r\n\r\n```\r\n{'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\r\n\r\n```\r\n\r\n...... ç»§ç»­è¿­ä»£ç›´åˆ°è¾¾åˆ°é¢„è®¾çš„ subwords è¯è¡¨å¤§å°æˆ–ä¸‹ä¸€ä¸ªæœ€é«˜é¢‘çš„å­—èŠ‚å¯¹å‡ºç°é¢‘ç‡ä¸º 1ã€‚è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†æ›´åŠ åˆé€‚çš„è¯è¡¨ï¼Œè¿™ä¸ªè¯è¡¨å¯èƒ½ä¼šå‡ºç°ä¸€äº›ä¸æ˜¯å•è¯çš„ç»„åˆï¼Œä½†æ˜¯å…¶æœ¬èº«æœ‰æ„ä¹‰çš„ä¸€ç§å½¢å¼\r\n\r\nåœæ­¢ç¬¦Â `</w>`Â çš„æ„ä¹‰åœ¨äºè¡¨ç¤º subword æ˜¯è¯åç¼€ã€‚ä¸¾ä¾‹æ¥è¯´ï¼š`st`Â ä¸åŠ Â `</w>`Â å¯ä»¥å‡ºç°åœ¨è¯é¦–ï¼Œå¦‚Â `st ar`ï¼›åŠ äº†Â `</w>`Â è¡¨æ˜æ”¹å­—è¯ä½äºè¯å°¾ï¼Œå¦‚Â `wide st</w>`ï¼ŒäºŒè€…æ„ä¹‰æˆªç„¶ä¸åŒ\r\n\r\n### **BPE å®ç°**\r\n\r\n```\r\nimport re, collections\r\n\r\ndefget_vocab(filename):\r\n\r\n    vocab = collections.defaultdict(int)\r\n\r\nwith open(filename, 'r', encoding='utf-8')as fhand:\r\n\r\nfor linein fhand:\r\n\r\n            words = line.strip().split()\r\n\r\nfor wordin words:\r\n\r\n                vocab[' '.join(list(word)) + ' </w>'] += 1\r\n\r\nreturn vocab\r\n\r\ndefget_stats(vocab):\r\n\r\n    pairs = collections.defaultdict(int)\r\n\r\nfor word, freqin vocab.items():\r\n\r\n        symbols = word.split()\r\n\r\nfor iin range(len(symbols)-1):\r\n\r\n            pairs[symbols[i],symbols[i+1]] += freq\r\n\r\nreturn pairs\r\n\r\ndefmerge_vocab(pair, v_in):\r\n\r\n    v_out = {}\r\n\r\n    bigram = re.escape(' '.join(pair))\r\n\r\n    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\r\n\r\nfor wordin v_in:\r\n\r\n        w_out = p.sub(''.join(pair), word)\r\n\r\n        v_out[w_out] = v_in[word]\r\n\r\nreturn v_out\r\n\r\ndefget_tokens(vocab):\r\n\r\n    tokens = collections.defaultdict(int)\r\n\r\nfor word, freqin vocab.items():\r\n\r\n        word_tokens = word.split()\r\n\r\nfor tokenin word_tokens:\r\n\r\n            tokens[token] += freq\r\n\r\nreturn tokens\r\n\r\nvocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\r\n\r\n# Get free book from Gutenberg\r\n\r\n# wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt\r\n\r\n# vocab = get_vocab('pg16457.txt')\r\n\r\nprint('==========')\r\n\r\nprint('Tokens Before BPE')\r\n\r\ntokens = get_tokens(vocab)\r\n\r\nprint('Tokens: {}'.format(tokens))\r\n\r\nprint('Number of tokens: {}'.format(len(tokens)))\r\n\r\nprint('==========')\r\n\r\nnum_merges = 5\r\n\r\nfor iin range(num_merges):\r\n\r\n    pairs = get_stats(vocab)\r\n\r\nifnot pairs:\r\n\r\nbreak\r\n    best = max(pairs, key=pairs.get)\r\n\r\n    vocab = merge_vocab(best, vocab)\r\n\r\n    print('Iter: {}'.format(i))\r\n\r\n    print('Best pair: {}'.format(best))\r\n\r\n    tokens = get_tokens(vocab)\r\n\r\n    print('Tokens: {}'.format(tokens))\r\n\r\n    print('Number of tokens: {}'.format(len(tokens)))\r\n\r\n    print('==========')\r\n\r\n```\r\n\r\nè¾“å‡ºå¦‚ä¸‹\r\n\r\n```\r\n==========\r\n\r\nTokens Before BPE\r\n\r\nTokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 16, 'e': 17, 'r': 2, 'n': 6, 's': 9, 't': 9, 'i': 3, 'd': 3})\r\n\r\nNumber of tokens: 11\r\n\r\n==========\r\n\r\nIter: 0\r\n\r\nBest pair: ('e', 's')\r\n\r\nTokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 16, 'e': 8, 'r': 2, 'n': 6, 'es': 9, 't': 9, 'i': 3, 'd': 3})\r\n\r\nNumber of tokens: 11\r\n\r\n==========\r\n\r\nIter: 1\r\n\r\nBest pair: ('es', 't')\r\n\r\nTokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 16, 'e': 8, 'r': 2, 'n': 6, 'est': 9, 'i': 3, 'd': 3})\r\n\r\nNumber of tokens: 10\r\n\r\n==========\r\n\r\nIter: 2\r\n\r\nBest pair: ('est', '</w>')\r\n\r\nTokens: defaultdict(<class 'int'>, {'l': 7, 'o': 7, 'w': 16, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'est</w>': 9, 'i': 3, 'd': 3})\r\n\r\nNumber of tokens: 10\r\n\r\n==========\r\n\r\nIter: 3\r\n\r\nBest pair: ('l', 'o')\r\n\r\nTokens: defaultdict(<class 'int'>, {'lo': 7, 'w': 16, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'est</w>': 9, 'i': 3, 'd': 3})\r\n\r\nNumber of tokens: 9\r\n\r\n==========\r\n\r\nIter: 4\r\n\r\nBest pair: ('lo', 'w')\r\n\r\nTokens: defaultdict(<class 'int'>, {'low': 7, '</w>': 7, 'e': 8, 'r': 2, 'n': 6, 'w': 9, 'est</w>': 9, 'i': 3, 'd': 3})\r\n\r\nNumber of tokens: 9\r\n\r\n==========\r\n\r\n```\r\n\r\n### **ç¼–ç å’Œè§£ç **\r\n\r\n### **ç¼–ç **\r\n\r\nåœ¨ä¹‹å‰çš„ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å·²ç»å¾—åˆ°äº† subword çš„è¯è¡¨ï¼Œå¯¹è¯¥è¯è¡¨æŒ‰ç…§å­—ç¬¦ä¸ªæ•°ç”±å¤šåˆ°å°‘æ’åºã€‚ç¼–ç æ—¶ï¼Œå¯¹äºæ¯ä¸ªå•è¯ï¼Œéå†æ’å¥½åºçš„å­è¯è¯è¡¨å¯»æ‰¾æ˜¯å¦æœ‰ token æ˜¯å½“å‰å•è¯çš„å­å­—ç¬¦ä¸²ï¼Œå¦‚æœæœ‰ï¼Œåˆ™è¯¥ token æ˜¯è¡¨ç¤ºå•è¯çš„ tokens ä¹‹ä¸€\r\n\r\næˆ‘ä»¬ä»æœ€é•¿çš„ token è¿­ä»£åˆ°æœ€çŸ­çš„ tokenï¼Œå°è¯•å°†æ¯ä¸ªå•è¯ä¸­çš„å­å­—ç¬¦ä¸²æ›¿æ¢ä¸º tokenã€‚ æœ€ç»ˆï¼Œæˆ‘ä»¬å°†è¿­ä»£æ‰€æœ‰ tokensï¼Œå¹¶å°†æ‰€æœ‰å­å­—ç¬¦ä¸²æ›¿æ¢ä¸º tokensã€‚ å¦‚æœä»ç„¶æœ‰å­å­—ç¬¦ä¸²æ²¡è¢«æ›¿æ¢ä½†æ‰€æœ‰ token éƒ½å·²è¿­ä»£å®Œæ¯•ï¼Œåˆ™å°†å‰©ä½™çš„å­è¯æ›¿æ¢ä¸ºç‰¹æ®Š tokenï¼Œå¦‚Â `<unk>`\r\n\r\nä¾‹å¦‚\r\n\r\n```\r\n# ç»™å®šå•è¯åºåˆ—\r\n\r\n[\"the</w>\", \"highest</w>\", \"mountain</w>\"]\r\n\r\n# æ’å¥½åºçš„subwordè¡¨\r\n\r\n# é•¿åº¦ 6         5           4        4         4       4          2\r\n\r\n[\"errrr</w>\", \"tain</w>\", \"moun\", \"est</w>\", \"high\", \"the</w>\", \"a</w>\"]\r\n\r\n# è¿­ä»£ç»“æœ\r\n\r\n\"the</w>\" -> [\"the</w>\"]\r\n\r\n\"highest</w>\" -> [\"high\", \"est</w>\"]\r\n\r\n\"mountain</w>\" -> [\"moun\", \"tain</w>\"]\r\n\r\n```\r\n\r\n### **è§£ç **\r\n\r\nå°†æ‰€æœ‰çš„ tokens æ‹¼åœ¨ä¸€èµ·å³å¯ï¼Œä¾‹å¦‚\r\n\r\n```\r\n# ç¼–ç åºåˆ—\r\n\r\n[\"the</w>\", \"high\", \"est</w>\", \"moun\", \"tain</w>\"]\r\n\r\n# è§£ç åºåˆ—\r\n\r\n\"the</w> highest</w> mountain</w>\"\r\n\r\n```\r\n\r\n### **ç¼–ç å’Œè§£ç å®ç°**\r\n\r\n```\r\nimport re, collections\r\n\r\ndefget_vocab(filename):\r\n\r\n    vocab = collections.defaultdict(int)\r\n\r\nwith open(filename, 'r', encoding='utf-8')as fhand:\r\n\r\nfor linein fhand:\r\n\r\n            words = line.strip().split()\r\n\r\nfor wordin words:\r\n\r\n                vocab[' '.join(list(word)) + ' </w>'] += 1\r\n\r\nreturn vocab\r\n\r\ndefget_stats(vocab):\r\n\r\n    pairs = collections.defaultdict(int)\r\n\r\nfor word, freqin vocab.items():\r\n\r\n        symbols = word.split()\r\n\r\nfor iin range(len(symbols)-1):\r\n\r\n            pairs[symbols[i],symbols[i+1]] += freq\r\n\r\nreturn pairs\r\n\r\ndefmerge_vocab(pair, v_in):\r\n\r\n    v_out = {}\r\n\r\n    bigram = re.escape(' '.join(pair))\r\n\r\n    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\r\n\r\nfor wordin v_in:\r\n\r\n        w_out = p.sub(''.join(pair), word)\r\n\r\n        v_out[w_out] = v_in[word]\r\n\r\nreturn v_out\r\n\r\ndefget_tokens_from_vocab(vocab):\r\n\r\n    tokens_frequencies = collections.defaultdict(int)\r\n\r\n    vocab_tokenization = {}\r\n\r\nfor word, freqin vocab.items():\r\n\r\n        word_tokens = word.split()\r\n\r\nfor tokenin word_tokens:\r\n\r\n            tokens_frequencies[token] += freq\r\n\r\n        vocab_tokenization[''.join(word_tokens)] = word_tokens\r\n\r\nreturn tokens_frequencies, vocab_tokenization\r\n\r\ndefmeasure_token_length(token):\r\n\r\nif token[-4:] == '</w>':\r\n\r\nreturn len(token[:-4]) + 1\r\n\r\nelse:\r\n\r\nreturn len(token)\r\n\r\ndeftokenize_word(string, sorted_tokens, unknown_token='</u>'):\r\n\r\nif string == '':\r\n\r\nreturn []\r\n\r\nif sorted_tokens == []:\r\n\r\nreturn [unknown_token]\r\n\r\n    string_tokens = []\r\n\r\nfor iin range(len(sorted_tokens)):\r\n\r\n        token = sorted_tokens[i]\r\n\r\n        token_reg = re.escape(token.replace('.', '[.]'))\r\n\r\n        matched_positions = [(m.start(0), m.end(0))for min re.finditer(token_reg, string)]\r\n\r\nif len(matched_positions) == 0:\r\n\r\ncontinue\r\n        substring_end_positions = [matched_position[0]for matched_positionin matched_positions]\r\n\r\n        substring_start_position = 0\r\n\r\nfor substring_end_positionin substring_end_positions:\r\n\r\n            substring = string[substring_start_position:substring_end_position]\r\n\r\n            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\r\n\r\n            string_tokens += [token]\r\n\r\n            substring_start_position = substring_end_position + len(token)\r\n\r\n        remaining_substring = string[substring_start_position:]\r\n\r\n        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\r\n\r\nbreak\r\nreturn string_tokens\r\n\r\n# vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\r\n\r\nvocab = get_vocab('pg16457.txt')\r\n\r\nprint('==========')\r\n\r\nprint('Tokens Before BPE')\r\n\r\ntokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\r\n\r\nprint('All tokens: {}'.format(tokens_frequencies.keys()))\r\n\r\nprint('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\r\n\r\nprint('==========')\r\n\r\nnum_merges = 10000\r\n\r\nfor iin range(num_merges):\r\n\r\n    pairs = get_stats(vocab)\r\n\r\nifnot pairs:\r\n\r\nbreak\r\n    best = max(pairs, key=pairs.get)\r\n\r\n    vocab = merge_vocab(best, vocab)\r\n\r\n    print('Iter: {}'.format(i))\r\n\r\n    print('Best pair: {}'.format(best))\r\n\r\n    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\r\n\r\n    print('All tokens: {}'.format(tokens_frequencies.keys()))\r\n\r\n    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\r\n\r\n    print('==========')\r\n\r\n# Let's check how tokenization will be for a known word\r\n\r\nword_given_known = 'mountains</w>'\r\n\r\nword_given_unknown = 'Ilikeeatingapples!</w>'\r\n\r\nsorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\r\n\r\nsorted_tokens = [tokenfor (token, freq)in sorted_tokens_tuple]\r\n\r\nprint(sorted_tokens)\r\n\r\nword_given = word_given_known\r\n\r\nprint('Tokenizing word: {}...'.format(word_given))\r\n\r\nif word_givenin vocab_tokenization:\r\n\r\n    print('Tokenization of the known word:')\r\n\r\n    print(vocab_tokenization[word_given])\r\n\r\n    print('Tokenization treating the known word as unknown:')\r\n\r\n    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\r\n\r\nelse:\r\n\r\n    print('Tokenizating of the unknown word:')\r\n\r\n    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\r\n\r\nword_given = word_given_unknown\r\n\r\nprint('Tokenizing word: {}...'.format(word_given))\r\n\r\nif word_givenin vocab_tokenization:\r\n\r\n    print('Tokenization of the known word:')\r\n\r\n    print(vocab_tokenization[word_given])\r\n\r\n    print('Tokenization treating the known word as unknown:')\r\n\r\n    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\r\n\r\nelse:\r\n\r\n    print('Tokenizating of the unknown word:')\r\n\r\n    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))\r\n\r\n```\r\n\r\nè¾“å‡ºå¦‚ä¸‹\r\n\r\n```\r\nTokenizing word: mountains</w>...\r\n\r\nTokenizationof the known word:\r\n\r\n['mountains</w>']\r\n\r\nTokenization treating the known wordas unknown:\r\n\r\n['mountains</w>']\r\n\r\nTokenizing word: Ilikeeatingapples!</w>...\r\n\r\nTokenizatingof the unknown word:\r\n\r\n['I', 'like', 'ea', 'ting', 'app', 'l', 'es!</w>']\r\n\r\n```\r\n\r\n### **Reference**\r\n\r\n- **[3 subword algorithms help to improve your NLP model performance](https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46)**\r\n- **[Tokenizers: How machines read](https://blog.floydhub.com/tokenization-nlp/)**\r\n- **[Overview of tokenization algorithms in NLP](https://towardsdatascience.com/overview-of-nlp-tokenization-algorithms-c41a7d5ec4f9)**\r\n- **[ä¸€æ–‡è¯»æ‡‚ BERT ä¸­çš„ WordPiece](https://www.cnblogs.com/huangyc/p/10223075.html)**\r\n- **[Byte Pair Encoding](https://leimao.github.io/blog/Byte-Pair-Encoding/)**\r\n- **[æ·±å…¥ç†è§£ NLP Subword ç®—æ³•ï¼šBPEã€WordPieceã€ULM](https://zhuanlan.zhihu.com/p/86965595)**",
      "data": {
        "title": "BPE ç®—æ³•è¯¦è§£",
        "date": "2020-02-03 09:19:23",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "bpe-suan-fa-xiang-jie"
    },
    {
      "content": "ä¸€ã€å¼‚å¸¸é—®é¢˜\r\n===============\r\n## Gitalk-è®¾ç½®åè¿æ¥403åŸå› ï¼š\r\n![](https://zhousanfu.github.io/post-images/1614321281694.png)\r\nè§£å†³æ–¹æ³•ï¼šè®¿é—®-[ç”³è¯·è§£å¼€å°é”](https://cors-anywhere.herokuapp.com/corsdemo)\r\nè¿”å›ï¼šYou currently have temporary access to the demo serverï¼Œå°±å¯ã€‚\r\n\r\n--å®˜æ–¹æ–‡æ¡£è§£é‡Šï¼šhttps://github.com/Rob--W/cors-anywhere/issues/301\r\n",
      "data": {
        "title": "GithubPagesä¸Gitalk çš„æ€»ç»“ä¸å¼‚å¸¸",
        "date": "2020-01-26 14:13:35",
        "tags": [
          "å‰ç«¯",
          "Gridea"
        ],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "gitalk-de-zong-jie-yu-yi-chang"
    },
    {
      "content": "æ–°æ‰‹å‹å¥½ [Markdown å…¥é—¨å‚è€ƒ](https://xianbai.me/learn-md/index.html)",
      "data": {
        "title": "Learning-Markdown (Markdown å…¥é—¨å‚è€ƒ)",
        "date": "2019-02-26 16:46:00",
        "tags": [],
        "published": true,
        "hideInList": false,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "learning-markdown-markdown-ru-men-can-kao"
    },
    {
      "content": "ğŸ‘  æ¬¢è¿ä½¿ç”¨ **Gridea** ï¼  \nâœï¸  **Gridea** ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ... \n\n<!-- more -->\n\n[Github](https://github.com/getgridea/gridea)  \n[Gridea ä¸»é¡µ](https://gridea.dev/)  \n[ç¤ºä¾‹ç½‘ç«™](http://fehey.com/)\n\n## ç‰¹æ€§ğŸ‘‡\nğŸ“  ä½ å¯ä»¥ä½¿ç”¨æœ€é…·çš„ **Markdown** è¯­æ³•ï¼Œè¿›è¡Œå¿«é€Ÿåˆ›ä½œ  \n\nğŸŒ‰  ä½ å¯ä»¥ç»™æ–‡ç« é…ä¸Šç²¾ç¾çš„å°é¢å›¾å’Œåœ¨æ–‡ç« ä»»æ„ä½ç½®æ’å…¥å›¾ç‰‡  \n\nğŸ·ï¸  ä½ å¯ä»¥å¯¹æ–‡ç« è¿›è¡Œæ ‡ç­¾åˆ†ç»„  \n\nğŸ“‹  ä½ å¯ä»¥è‡ªå®šä¹‰èœå•ï¼Œç”šè‡³å¯ä»¥åˆ›å»ºå¤–éƒ¨é“¾æ¥èœå•  \n\nğŸ’»  ä½ å¯ä»¥åœ¨ **Windows**ï¼Œ**MacOS** æˆ– **Linux** è®¾å¤‡ä¸Šä½¿ç”¨æ­¤å®¢æˆ·ç«¯  \n\nğŸŒ  ä½ å¯ä»¥ä½¿ç”¨ **ğ–¦ğ—‚ğ—ğ—ğ—ğ–» ğ–¯ğ–ºğ—€ğ–¾ğ—Œ** æˆ– **Coding Pages** å‘ä¸–ç•Œå±•ç¤ºï¼Œæœªæ¥å°†æ”¯æŒæ›´å¤šå¹³å°  \n\nğŸ’¬  ä½ å¯ä»¥è¿›è¡Œç®€å•çš„é…ç½®ï¼Œæ¥å…¥ [Gitalk](https://github.com/gitalk/gitalk) æˆ– [DisqusJS](https://github.com/SukkaW/DisqusJS) è¯„è®ºç³»ç»Ÿ  \n\nğŸ‡¬ğŸ‡§  ä½ å¯ä»¥ä½¿ç”¨**ä¸­æ–‡ç®€ä½“**æˆ–**è‹±è¯­**  \n\nğŸŒ  ä½ å¯ä»¥ä»»æ„ä½¿ç”¨åº”ç”¨å†…é»˜è®¤ä¸»é¢˜æˆ–ä»»æ„ç¬¬ä¸‰æ–¹ä¸»é¢˜ï¼Œå¼ºå¤§çš„ä¸»é¢˜è‡ªå®šä¹‰èƒ½åŠ›  \n\nğŸ–¥  ä½ å¯ä»¥è‡ªå®šä¹‰æºæ–‡ä»¶å¤¹ï¼Œåˆ©ç”¨ OneDriveã€ç™¾åº¦ç½‘ç›˜ã€iCloudã€Dropbox ç­‰è¿›è¡Œå¤šè®¾å¤‡åŒæ­¥  \n\nğŸŒ± å½“ç„¶ **Gridea** è¿˜å¾ˆå¹´è½»ï¼Œæœ‰å¾ˆå¤šä¸è¶³ï¼Œä½†è¯·ç›¸ä¿¡ï¼Œå®ƒä¼šä¸åœå‘å‰ ğŸƒ\n\næœªæ¥ï¼Œå®ƒä¸€å®šä¼šæˆä¸ºä½ ç¦»ä¸å¼€çš„ä¼™ä¼´\n\nå°½æƒ…å‘æŒ¥ä½ çš„æ‰åå§ï¼\n\nğŸ˜˜ Enjoy~\n",
      "data": {
        "title": "Hello Gridea",
        "date": "2019-02-05 09:05:06",
        "tags": [
          "Gridea"
        ],
        "published": true,
        "hideInList": false,
        "feature": "/post-images/hello-gridea.png",
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "ğŸ‘  æ¬¢è¿ä½¿ç”¨ **Gridea** ï¼  \nâœï¸  **Gridea** ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ... ",
      "fileName": "hello-gridea"
    },
    {
      "content": "> æ¬¢è¿æ¥åˆ°æˆ‘çš„å°ç«™å‘€ï¼Œå¾ˆé«˜å…´é‡è§ä½ ï¼ğŸ¤\n\n## ğŸ  å…³äºæœ¬ç«™\nå­¦ä¹ ç¬”è®°\n\n## ğŸ‘¨â€ğŸ’» åšä¸»æ˜¯è°\nå‘¨ä¸‰ç”«\n\n## â›¹ å…´è¶£çˆ±å¥½\n\n## ğŸ“¬ è”ç³»æˆ‘å‘€\n\n",
      "data": {
        "title": "å…³äº",
        "date": "2019-01-25 19:09:48",
        "tags": [],
        "published": true,
        "hideInList": true,
        "feature": null,
        "isTop": false
      },
      "isEmpty": false,
      "excerpt": "",
      "abstract": "",
      "fileName": "about"
    }
  ],
  "tags": [
    {
      "index": -1,
      "name": "Tensorflow",
      "slug": "Tensorflow",
      "used": true
    },
    {
      "index": -1,
      "name": "æ•°æ®æŒ–æ˜",
      "slug": "data-grub",
      "used": true
    },
    {
      "index": -1,
      "name": "NLP",
      "slug": "nlp",
      "used": true
    },
    {
      "index": -1,
      "name": "å‰ç«¯",
      "slug": "front_end",
      "used": true
    },
    {
      "name": "Gridea",
      "slug": "xyBbGRwYX",
      "used": true
    }
  ],
  "menus": [
    {
      "link": "/",
      "name": "é¦–é¡µ",
      "openType": "Internal"
    },
    {
      "link": "/archives",
      "name": "å½’æ¡£",
      "openType": "Internal"
    },
    {
      "link": "/tags",
      "name": "æ ‡ç­¾",
      "openType": "Internal"
    },
    {
      "link": "/post/about",
      "name": "å…³äº",
      "openType": "Internal"
    }
  ]
}